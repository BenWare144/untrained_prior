{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dbdbd90-fe8d-49ca-a07b-6899edbab37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Local imports\n",
    "from my_utils import *\n",
    "\n",
    "\n",
    "### Example usage:\n",
    "# PM = ProjectManager()\n",
    "# data = PM.fetch_data_by_label(\"activations\", \"double_2\")\n",
    "\n",
    "### saving\n",
    "# PM.save_dataset(\"word_features\", PM.fn_word_frequencies, data=word_frequencies)\n",
    "# PM.get_files(\"transformer_weights\")\n",
    "\n",
    "### loading\n",
    "# sentence_features = PM.load_dataset(\"sentence_features\")\n",
    "# sentence_features\n",
    "# word_features = PM.load_dataset(\"word_features\")\n",
    "# word_features\n",
    "# gpt_input = PM.load_dataset(\"gpt_input\")\n",
    "# gpt_input\n",
    "\n",
    "class ProjectManager:\n",
    "    \"\"\"\n",
    "    Manages project directories, data loading, saving operations, and mappings for models and Part-of-Speech (POS) tags.\n",
    "\n",
    "    Attributes:\n",
    "        Xss (dict): A dictionary storing activation data for various models and layers.\n",
    "        Xss_6 (dict): Similar to `Xss`, but with data compressed to the POS-6 level of granularity.\n",
    "        ys (dict): A dictionary storing target variables for various classification and regression tasks.\n",
    "        ys_6 (dict): Similar to `ys`, but with data compressed to the POS-6 level.\n",
    "        is_in_POS_6 (list): A boolean list indicating which data points are included in the POS-6 tagset.\n",
    "\n",
    "        file_cache (dict): A cache of loaded files to prevent reloading data multiple times, improving performance.\n",
    "\n",
    "        base_dir (Path): The base directory for the project data, defaulting to the user's home directory.\n",
    "        directories (dict): A mapping of data categories (e.g., 'transformer_weights', 'word_features') to their corresponding directories within the project.\n",
    "\n",
    "        fn_base_sent_features (str): Filename for base sentence features.\n",
    "        fn_base_word_features (str): Filename for base word features.\n",
    "        fn_word_indexes (str): Filename for word indexes.\n",
    "        fn_pos_features (str): Filename for POS features.\n",
    "        fn_word_frequencies (str): Filename for word frequencies.\n",
    "        fn_tree_depth (str): Filename for tree depth data.\n",
    "        fn_function (str): Filename for function data (e.g., function vs. content words).\n",
    "\n",
    "        sample_idx_min (int): Starting word index of conll2012_ontonotesv5 sample.\n",
    "        sample_idx_max (int): Ending word index of conll2012_ontonotesv5 sample.\n",
    "        sels (list): A list of indices used in analysis.\n",
    "        maxps (list): A list of indices used in analysis.\n",
    "\n",
    "        all_models (list): A list of all models used in the project (e.g., 'gpt2', 'gpt2-untrained').\n",
    "        gpt2_models (list): List of GPT-2 models used in the project.\n",
    "        gpt2xl_models (list): List of GPT-2 XL models used in the project.\n",
    "        model_label_map (dict): A mapping between internal model names and their display labels (e.g., 'gpt2-xl' -> 'XL-Trained').\n",
    "        model_group_map (dict): A mapping of models to their respective training groups (e.g., 'Trained', 'Untrained', 'Gaussian') for analysis.\n",
    "        model_groups (dict): A mapping of models to their respective wieght groups (e.g., 'main', 'single', 'doubles', and 'quads') for analysis.\n",
    "\n",
    "        layers_gpt2xl (list): A list of layer names for the GPT-2 XL model, including the \"drop\" layer.\n",
    "        layers_gpt2 (list): A list of layer names for the GPT-2 model, derived from `layers_gpt2xl`.\n",
    "        layers_dict (dict): A mapping of models to their respective layer lists (e.g., GPT-2 or GPT-2 XL layers).\n",
    "        layers_to_idx (dict): A dictionary mapping layer names to their indices for easier lookup.\n",
    "\n",
    "        POS_51_all_tags (list): A list of all 51 POS tags used in the project.\n",
    "        POS_12_all_tags (list): A list of 12 broader POS tags for a higher-level classification.\n",
    "        POS_7_all_tags (list): A list of 7 even broader POS tags, with some tags collapsed into categories like 'X' for unknown or unclassifiable tags.\n",
    "        POS_6_all_tags (list): A list of 6 POS tags, representing the most basic categorization (e.g., Noun, Verb).\n",
    "\n",
    "        POS_12_to_POS_7 (dict): A mapping from the 12-tag POS set to the 7-tag set (e.g., 'NOUN' -> 'Noun').\n",
    "        POS_51_tag_to_id (dict): A mapping from the 51-tag POS set to their corresponding indices.\n",
    "        POS_12_tag_to_id (dict): A mapping from the 12-tag POS set to their corresponding indices.\n",
    "        POS_7_tag_to_id (dict): A mapping from the 7-tag POS set to their corresponding indices.\n",
    "        POS_6_tag_to_id (dict): A mapping from the 6-tag POS set to their corresponding indices.\n",
    "\n",
    "        pos_names (list): POS tag names\n",
    "\n",
    "        classification_labels (list): A list of all classification labels.\n",
    "        my_classification_targets (list): The subset of the classification labels used for analysis. (and for Next, Current, and Previous words)\n",
    "        reggression_labels (list): Labels for regression tasks. (and for Next, Current, and Previous words)\n",
    "        pos_labels (list): All part-of-speech targets for classification tasks. (and for Next, Current, and Previous words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the ProjectManager with default settings, directories, \n",
    "        model mappings, and POS tag configurations.\n",
    "        \"\"\"\n",
    "        # Initialize model activations and targets dictionaries\n",
    "        self.Xss = {}\n",
    "        self.ys = {}\n",
    "        self.ys_6 = {}\n",
    "        self.Xss_6 = {}\n",
    "        self.is_in_POS_6 = []\n",
    "\n",
    "        # File caching for quicker data access\n",
    "        self.file_cache = {}\n",
    "\n",
    "        # Base directory for project data (default to user's HOME directory)\n",
    "        self.base_dir = Path(os.getenv(\"HOME\")) / \"data\"\n",
    "\n",
    "        # Initialize directory structure for various data categories\n",
    "        self.directories = {\n",
    "            \"transformer_weights\": self.base_dir / \"transformer_weights\",\n",
    "            \"word_features\": self.base_dir / \"word_features\",\n",
    "            \"sentence_features\": self.base_dir / \"sentence_features\",\n",
    "            \"scores\": self.base_dir / \"scores\",\n",
    "            \"activations\": self.base_dir / \"activations\",\n",
    "            \"context_lengths\": self.base_dir / \"context_lengths\",\n",
    "            \"analysis_res\": self.base_dir / \"analysis_res\",\n",
    "            \"thesis_tex\": self.base_dir / \"thesis_tex\",\n",
    "            \"tbl\": self.base_dir / \"thesis_tex\" / \"tbl\",\n",
    "            \"fig\": self.base_dir / \"thesis_tex\" / \"fig\",\n",
    "            \"fimg\": self.base_dir / \"thesis_tex\" / \"fimg\",\n",
    "            \"hf_files\": self.base_dir / \"hf_files\",\n",
    "            \"gpt_input\": self.base_dir / \"gpt_input\",\n",
    "            \"exp1\": self.base_dir / \"experiment_1_results\",\n",
    "            \"exp2\": self.base_dir / \"experiment_2_results\",\n",
    "            \"exp3\": self.base_dir / \"experiment_3_results\",\n",
    "            \"exp3_test\": self.base_dir / \"experiment_3_test\",\n",
    "        }\n",
    "\n",
    "        # Initialize filenames for specific datasets\n",
    "        self.fn_base_sent_features = \"base_sentence_features.csv\"\n",
    "        self.fn_base_word_features = \"base_word_features.csv\"\n",
    "        self.fn_word_indexes = \"word_indexes.csv\"\n",
    "        self.fn_pos_features = \"pos_features.csv\"\n",
    "        self.fn_word_frequencies = \"word_frequencies.csv\"\n",
    "        self.fn_tree_depth = \"tree_depth.csv\"\n",
    "        self.fn_function = \"function.csv\"\n",
    "\n",
    "        # Sample index boundaries\n",
    "        self.sample_idx_min = 386  # Starting index for samples\n",
    "        self.sample_idx_max = 8344  # Ending index for samples\n",
    "\n",
    "        # Selection indices for analysis\n",
    "        self.sels = [f\"sel_{i}\" for i in range(6)]\n",
    "        self.maxps = [f\"maxp_{i}\" for i in range(6)]\n",
    "\n",
    "        # Initialize models, layers, and label mappings\n",
    "        self._initialize_model_mappings()\n",
    "\n",
    "        # Initialize POS tags and mappings\n",
    "        self._initialize_pos_tags()\n",
    "\n",
    "        # Initialize labels for regression and classification targets\n",
    "        self._initialize_analysis_labels()\n",
    "\n",
    "    def _initialize_model_mappings(self):\n",
    "        \"\"\"\n",
    "        Initializes model labels, layer mappings, and groupings for different models.\n",
    "        \"\"\"\n",
    "        # Define all models and map their internal labels to display labels\n",
    "        self.all_models = [\n",
    "            'gpt2-xl', 'gpt2-xl-untrained_1', 'gpt2',\n",
    "            *[f\"gpt2-untrained_{i}\" for i in range(1, 10)],\n",
    "            *[f\"gpt2-untrained_{i}_weight_config_all\" for i in range(1, 10)]\n",
    "        ]\n",
    "        self.gpt2xl_models = self.all_models[:2]  # First two models are GPT-2 XL\n",
    "        self.gpt2_models = self.all_models[2:]    # Remaining models are GPT-2\n",
    "        # Mapping between model labels and their display names\n",
    "        self.model_label_map = {\n",
    "            'gpt2-xl': 'XL-Trained',\n",
    "            'gpt2-xl-untrained_1': 'XL-Untrained_1',\n",
    "            'gpt2': 'Trained',\n",
    "            **{f\"gpt2-untrained_{i}\": f\"Untrained_{i}\" for i in range(1, 10)},\n",
    "            **{f\"gpt2-untrained_{i}_weight_config_all\": f\"Gaussian_{i}\" for i in range(1, 10)}\n",
    "        }\n",
    "\n",
    "        # Grouping models for structured analysis\n",
    "        self.model_group_map = {\n",
    "            'gpt2-xl': 'XL-Trained',\n",
    "            'gpt2-xl-untrained_1': 'XL-Untrained',\n",
    "            'gpt2': 'Trained',\n",
    "            **{f\"gpt2-untrained_{i}\": 'Untrained' for i in range(1, 10)},\n",
    "            **{f\"gpt2-untrained_{i}_weight_config_all\": 'Gaussian' for i in range(1, 10)},\n",
    "        }\n",
    "\n",
    "        # Model wight grouping setup (for now deleted expiriment)\n",
    "        self.model_groups = {\n",
    "            \"main\": [\"gpt2\"] + [f\"gpt2-untrained_{i}\" for i in range(10)] + [\"all\"],\n",
    "            \"single\": [f\"single_{i}\" for i in range(12)],\n",
    "            \"doubles\": [f\"double_{i}\" for i in range(6)],\n",
    "            \"quads\": ['attns', 'mlps', 'lns'],\n",
    "        }\n",
    "\n",
    "        # Layer definitions for each model\n",
    "        self.layers_gpt2xl = [\"drop\"] + [f'encoder.h.{i}' for i in range(48)]\n",
    "        self.layers_gpt2 = self.layers_gpt2xl[:13]\n",
    "\n",
    "        # Map models to their respective layer configurations\n",
    "        self.layers_dict = {\n",
    "            **{model: self.layers_gpt2xl for model in ['gpt2-xl', 'gpt2-xl-untrained_1']},\n",
    "            **{model: self.layers_gpt2 for model in ['gpt2', *[f'gpt2-untrained_{i}' for i in range(1, 10)]]}\n",
    "        }\n",
    "\n",
    "        # Mapping from layer names to their indices\n",
    "        self.layers_to_idx = {layer: idx for idx, layer in enumerate(self.layers_gpt2xl)}\n",
    "\n",
    "    def _initialize_pos_tags(self):\n",
    "        \"\"\"\n",
    "        Initializes Part-of-Speech (POS) tags and related mappings for various levels of granularity.\n",
    "        \"\"\"\n",
    "        # POS tag levels and their mappings for classification tasks\n",
    "        self.POS_51_all_tags = [\n",
    "            \"XX\", \"``\", \"$\", \"''\", \"*\", \",\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"ADD\", \"AFX\", \"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"HYPH\", \"IN\",\n",
    "            \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NFP\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\",\n",
    "            \"SYM\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"VERB\", \"WDT\", \"WP\", \"WP$\", \"WRB\"\n",
    "        ]\n",
    "        self.POS_12_all_tags = [\"VERB\", \"NOUN\", \"PRON\", \"ADJ\", \"ADV\", \"ADP\", \"CONJ\", \"DET\", \"NUM\", \"PRT\", \"X\", \".\"]\n",
    "        self.POS_7_all_tags = [\"Noun\", \"Verb\", \"Adposition\", \"Determiner\", \"Adjective\", \"Adverb\", \"X\"]\n",
    "        self.POS_6_all_tags = [\"Noun\", \"Verb\", \"Adposition\", \"Determiner\", \"Adjective\", \"Adverb\"]\n",
    "\n",
    "        # Mappings from broader to narrower POS tags\n",
    "        self.POS_12_to_POS_7 = {\n",
    "            \"NOUN\": \"Noun\", \"PRON\": \"Noun\", \"VERB\": \"Verb\", \"ADP\": \"Adposition\",\n",
    "            \"DET\": \"Determiner\", \"ADJ\": \"Adjective\", \"ADV\": \"Adverb\", \"CONJ\": \"X\",\n",
    "            \"NUM\": \"X\", \"PRT\": \"X\", \"X\": \"X\", \".\": \"X\"\n",
    "        }\n",
    "\n",
    "        # Tag to ID mappings for easy lookup\n",
    "        self.POS_51_tag_to_id = {tag: idx for idx, tag in enumerate(self.POS_51_all_tags)}\n",
    "        self.POS_12_tag_to_id = {tag: idx for idx, tag in enumerate(self.POS_12_all_tags)}\n",
    "        self.POS_7_tag_to_id = {tag: idx for idx, tag in enumerate(self.POS_7_all_tags)}\n",
    "        self.POS_6_tag_to_id = {tag: idx for idx, tag in enumerate(self.POS_6_all_tags)}\n",
    "\n",
    "        # POS tag names\n",
    "        self.pos_names = [\"XX\", \"``\", \"$\", \"''\", \"*\", \",\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"ADD\", \"AFX\", \"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \n",
    "                          \"HYPH\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NFP\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"PDT\", \"POS\", \"PRP\", \n",
    "                          \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"VERB\", \n",
    "                          \"WDT\", \"WP\", \"WP$\", \"WRB\"]\n",
    "\n",
    "    def _initialize_analysis_labels(self):\n",
    "        \"\"\"\n",
    "        Initialize label names for regression and classification targets\n",
    "        \"\"\"\n",
    "\n",
    "        # Classification labels\n",
    "        self.classification_labels = []\n",
    "        for x in ['words', 'word_idx', 'predicate_lemmas', 'predicate_framenet_ids', 'word_senses', 'named_entities', 'function', 'tree_depth']:\n",
    "            self.classification_labels += [x, f\"{x}-\", f\"{x}+\"]\n",
    "\n",
    "        # Custom classification target labels\n",
    "        self.my_classification_targets = []\n",
    "        for x in ['function', 'tree_depth', 'word_idx']:\n",
    "            self.my_classification_targets += [x, f\"{x}-\", f\"{x}+\"]\n",
    "\n",
    "        # Regression labels\n",
    "        self.reggression_labels = []\n",
    "        for x in ['sentence_idx', 'unigram_probs', 'bigram_probs', 'trigram_probs']:\n",
    "            self.reggression_labels += [x, f\"{x}-\", f\"{x}+\"]\n",
    "\n",
    "        # POS labels for various tasks\n",
    "        self.pos_labels = []\n",
    "        for x in ['pos_tags', 'POS_12_id', 'POS_7_id']:\n",
    "            self.pos_labels += [x, f\"{x}-\", f\"{x}+\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _get_model_label(self, display_label):\n",
    "        \"\"\"\n",
    "        Convert a display label to the corresponding internal model label.\n",
    "\n",
    "        Args:\n",
    "            display_label (str): The display label of the model.\n",
    "\n",
    "        Returns:\n",
    "            str: The internal model label corresponding to the display label.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the display label is not recognized.\n",
    "        \"\"\"\n",
    "        for model_label, disp_label in self.model_label_map.items():\n",
    "            if disp_label == display_label or model_label == display_label:\n",
    "                return model_label\n",
    "        raise ValueError(f\"Unknown display label: {display_label}\")\n",
    "\n",
    "\n",
    "    def _get_display_label(self, model_label):\n",
    "        \"\"\"\n",
    "        Convert an internal model label to the corresponding display label.\n",
    "\n",
    "        Args:\n",
    "            model_label (str): The internal model label.\n",
    "\n",
    "        Returns:\n",
    "            str: The display label corresponding to the model label.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model label is not recognized.\n",
    "        \"\"\"\n",
    "        for _model_label, display_label in self.model_label_map.items():\n",
    "            if model_label == _model_label or model_label == display_label:\n",
    "                return display_label\n",
    "        raise ValueError(f\"Unknown display label: {display_label}\")\n",
    "\n",
    "\n",
    "    def get_base_and_full_labels(self, model_label):\n",
    "        \"\"\"\n",
    "        Get the base and full labels for a given model.\n",
    "\n",
    "        Args:\n",
    "            model_label (str): The internal model label.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple of (base_label, full_label) for the model.\n",
    "        \"\"\"\n",
    "        if model_label in [\"gpt2\", \"gpt2-untrained\", \"gpt2-xl\", \"gpt2-xl-untrained\"]:\n",
    "            base_label = model_label\n",
    "            full_label = model_label\n",
    "        elif \"_weight_config_\" in model_label:\n",
    "            base_label = \"gpt2-untrained\"\n",
    "            full_label = f\"{model_label}\"\n",
    "        else:\n",
    "            base_label = \"gpt2-untrained\"\n",
    "            full_label = f\"{base_label}_weight_config_{model_label}\"\n",
    "        return base_label, full_label\n",
    "\n",
    "\n",
    "    def check_for_existing_patterns(self, dir_key):\n",
    "        \"\"\"\n",
    "        Check for existing file patterns in the specified directory.\n",
    "\n",
    "        Args:\n",
    "            dir_key (str): The directory key.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of models for which file patterns exist in the directory.\n",
    "        \"\"\"\n",
    "        return [x for x in self.all_models if self.do_patterns_exist(dir_key, x)]\n",
    "\n",
    "\n",
    "    def do_patterns_exist(self, dir_key, model_label):\n",
    "        \"\"\"\n",
    "        Check if file patterns for a given model exist in the specified directory.\n",
    "\n",
    "        Args:\n",
    "            dir_key (str): The directory key.\n",
    "            model_label (str): The internal model label.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if file patterns exist, False otherwise.\n",
    "        \"\"\"\n",
    "        files = self.generate_file_patterns(dir_key, model_label)\n",
    "        if isinstance(files, tuple):\n",
    "            files = list(files)\n",
    "        else:\n",
    "            files = [files]\n",
    "        return all(Path(self.directories[dir_key], f).is_file() for f in files)\n",
    "\n",
    "\n",
    "    def generate_file_patterns(self, dir_key, model_label):\n",
    "        \"\"\"\n",
    "        Generate file patterns for a given model label and directory key.\n",
    "\n",
    "        Args:\n",
    "            dir_key (str): The directory key (e.g., 'activations').\n",
    "            model_label (str): The internal model label.\n",
    "\n",
    "        Returns:\n",
    "            str or tuple: The file pattern(s) associated with the given model and directory.\n",
    "        \"\"\"\n",
    "        base_label, full_label = self.get_base_and_full_labels(model_label)\n",
    "        if dir_key == \"activations\":\n",
    "            return (\n",
    "                f\"{full_label}__activations_v1_i0.npy\",\n",
    "                f\"{full_label}__activations_v1_i0_metadata.csv\",\n",
    "                f\"{full_label}_layers.txt\"\n",
    "            )\n",
    "        elif dir_key == \"scores\":\n",
    "            # The scores seem to have a timestamp, so we use a wildcard for that part\n",
    "            return f\"{full_label}_*score_raw.csv\"\n",
    "        elif dir_key == \"word_features\" or dir_key == \"sentence_features\":\n",
    "            # Assuming a generic pattern for word and sentence features; adjust as needed\n",
    "            return f\"{full_label}_*.csv\"\n",
    "        elif dir_key == \"transformer_saved_models\":\n",
    "            return f\"E1_Schrimpfs_{base_label}\"\n",
    "        elif dir_key == \"transformer_weights\":\n",
    "            return f\"{full_label}.pt\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported directory key for pattern generation: {dir_key}\")\n",
    "\n",
    "\n",
    "    def fetch_data_by_label(self, dir_key, display_label):\n",
    "        \"\"\"\n",
    "        Fetch data based on the directory key and display label.\n",
    "\n",
    "        Args:\n",
    "            dir_key (str): The directory key (e.g., 'activations').\n",
    "            display_label (str): The display label of the model.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of data files corresponding to the specified model and directory.\n",
    "        \"\"\"\n",
    "        model_label = self._get_model_label(display_label)\n",
    "        base_label, full_label = self.get_base_and_full_labels(model_label)\n",
    "\n",
    "        if dir_key == \"activations\":\n",
    "            pattern = f\"{full_label}__activations*.*\"\n",
    "        elif dir_key == \"scores\":\n",
    "            pattern = f\"{full_label}_*score_raw.csv\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported directory key for fetching: {dir_key}\")\n",
    "\n",
    "        files = self.get_files(dir_key, pattern)\n",
    "\n",
    "        if dir_key == \"activations\":\n",
    "            data = [np.load(file) for file in files if file.suffix == \".npy\"]\n",
    "        elif dir_key == \"scores\":\n",
    "            data = [pd.read_csv(file) for file in files]\n",
    "        else:\n",
    "            data = []\n",
    "        return data\n",
    "\n",
    "\n",
    "    def get_files(self, dir_key, pattern=\"*\"):\n",
    "        \"\"\"\n",
    "        Get all files in a directory matching a pattern.\n",
    "\n",
    "        Args:\n",
    "            dir_key (str): The directory key.\n",
    "            pattern (str): The pattern to match files (default is '*').\n",
    "\n",
    "        Returns:\n",
    "            list: A list of files matching the pattern in the specified directory.\n",
    "        \"\"\"\n",
    "        dir_path = self.directories.get(dir_key)\n",
    "        if not dir_path:\n",
    "            raise ValueError(f\"Unknown directory key: {dir_key}\")\n",
    "        return list(dir_path.glob(pattern))\n",
    "\n",
    "\n",
    "    def check_if_data_exists(self, dir_key, file_name):\n",
    "        \"\"\"\n",
    "        Check if a data file exists in the specified directory.\n",
    "\n",
    "        Args:\n",
    "            dir_key (str): The directory key.\n",
    "            file_name (str): The name of the file.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the file exists, False otherwise.\n",
    "        \"\"\"\n",
    "        file_path = self.directories[dir_key] / file_name\n",
    "        return file_path.exists()\n",
    "\n",
    "\n",
    "    def save_data(self, dir_key, file_name, data, use_cache=True):\n",
    "        \"\"\"\n",
    "        Save data to a file in the specified directory.\n",
    "\n",
    "        Args:\n",
    "            dir_key (str): The directory key.\n",
    "            file_name (str): The name of the file to save.\n",
    "            data: The data to save.\n",
    "            use_cache (bool): Whether to cache the file after saving (default is True).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the file type is unsupported.\n",
    "        \"\"\"\n",
    "        file_path = self.directories[dir_key] / file_name\n",
    "        if file_path.suffix == \".csv\":\n",
    "            if isinstance(data, dict):\n",
    "                pd.DataFrame.from_dict(data).to_csv(file_path, index=False)\n",
    "            elif isinstance(data, pd.DataFrame):\n",
    "                data.to_csv(file_path, index=False)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported data type for CSV: {type(data)}\")\n",
    "        elif file_path.suffix == \".npy\":\n",
    "            np.save(file_path, data)\n",
    "        elif file_path.suffix == \".txt\" and isinstance(data, list):\n",
    "            with open(file_path, 'w') as f:\n",
    "                for x in data:\n",
    "                    f.write(x + '\\n')\n",
    "        elif file_path.suffix == \".txt\":\n",
    "            with open(file_path, 'w') as f:\n",
    "                f.write(data)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_path.suffix}\")\n",
    "\n",
    "        if use_cache:\n",
    "            self.file_cache[file_name] = data\n",
    "\n",
    "\n",
    "    def load_data(self, dir_key, file_name, use_cache=False):\n",
    "        \"\"\"\n",
    "        Load data from a file, with optional caching.\n",
    "\n",
    "        Args:\n",
    "            dir_key (str): The directory key.\n",
    "            file_name (str): The name of the file to load.\n",
    "            use_cache (bool): Whether to use cached data if available (default is False).\n",
    "\n",
    "        Returns:\n",
    "            The loaded data, or None if the file does not exist.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the file type is unsupported.\n",
    "        \"\"\"\n",
    "        file_path = self.directories[dir_key] / file_name\n",
    "\n",
    "        if use_cache and file_name in self.file_cache:\n",
    "            return self.file_cache[file_name]\n",
    "\n",
    "        if not file_path.exists():\n",
    "            return None\n",
    "\n",
    "        if file_path.suffix == \".csv\":\n",
    "            data = pd.read_csv(file_path)\n",
    "        elif file_path.suffix == \".npy\":\n",
    "            data = np.load(file_path)\n",
    "        elif file_path.suffix == \".txt\" and \"layers\" in file_name:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = [line.strip() for line in f]\n",
    "        elif file_path.suffix == \".txt\":\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = f.read()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_path.suffix}\")\n",
    "\n",
    "        if use_cache:\n",
    "            self.file_cache[file_name] = data\n",
    "        return data\n",
    "\n",
    "    def save_dataset(self, dir_key, custom_name=None, data=None):\n",
    "        \"\"\"\n",
    "        Save non-model-specific datasets to a file using standardized naming conventions.\n",
    "\n",
    "        Args:\n",
    "            dir_key (str): The directory key (e.g., 'word_features', 'sentence_features', 'gpt_input').\n",
    "            custom_name (str, optional): A custom filename for the dataset (default is None).\n",
    "            data: The dataset to save.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the directory key is unsupported.\n",
    "        \"\"\"\n",
    "        if dir_key not in [\"word_features\", \"sentence_features\", \"gpt_input\"]:\n",
    "            raise ValueError(f\"Unsupported directory key for saving dataset: {dir_key}\")\n",
    "\n",
    "        file_name = custom_name if custom_name else f\"{dir_key}_dataset.csv\"\n",
    "        self.save_data(dir_key, file_name, data)\n",
    "\n",
    "\n",
    "    def load_dataset(self, dir_key, custom_name=None):\n",
    "        \"\"\"\n",
    "        Load non-model-specific datasets from a file using standardized naming conventions.\n",
    "\n",
    "        Args:\n",
    "            dir_key (str): The directory key (e.g., 'word_features', 'sentence_features', 'gpt_input').\n",
    "            custom_name (str, optional): A custom filename for the dataset (default is None).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The loaded dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the directory key is unsupported.\n",
    "        \"\"\"\n",
    "        if dir_key not in [\"word_features\", \"sentence_features\", \"gpt_input\"]:\n",
    "            raise ValueError(f\"Unsupported directory key for loading dataset: {dir_key}\")\n",
    "\n",
    "        if custom_name:\n",
    "            return self.load_data(dir_key, custom_name)\n",
    "        else:\n",
    "            return self.load_csvs_as_dataframe(dir_key)\n",
    "\n",
    "\n",
    "    def save_activations(self, activations, layers, model, metadata_df, use_cache=False):\n",
    "        \"\"\"\n",
    "        Save model activations, layers, and metadata to files.\n",
    "\n",
    "        Args:\n",
    "            activations: The activations data to save.\n",
    "            layers (list): The layers associated with the activations.\n",
    "            model (str): The model label.\n",
    "            metadata_df (pd.DataFrame): The metadata for the activations.\n",
    "            use_cache (bool): Whether to cache the saved files (default is False).\n",
    "        \"\"\"\n",
    "        activations_fn, metadata_fn, layers_fn = self.generate_file_patterns(\"activations\", model)\n",
    "        self.save_data(\"activations\", activations_fn, activations, use_cache=use_cache)\n",
    "        self.save_data(\"activations\", layers_fn, layers, use_cache=use_cache)\n",
    "        self.save_data(\"activations\", metadata_fn, metadata_df, use_cache=use_cache)\n",
    "\n",
    "\n",
    "    def load_activations(self, model, version=1, activations_indexes=[0], use_cache=False):\n",
    "        \"\"\"\n",
    "        Load model activations, layers, and metadata from files.\n",
    "\n",
    "        Args:\n",
    "            model (str): The model label.\n",
    "            version (int, optional): The version number of the activations (default is 1).\n",
    "            activations_indexes (list, optional): The indexes of the activations to load (default is [0]).\n",
    "            use_cache (bool): Whether to use cached activations if available (default is False).\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the activations (np.array), layers (list), and metadata (pd.DataFrame).\n",
    "        \"\"\"\n",
    "        all_activations = []\n",
    "        for activations_index in activations_indexes:\n",
    "            activations_fn, metadata_fn, layers_fn = self.generate_file_patterns(\"activations\", model)\n",
    "            all_activations.append(self.load_data(\"activations\", activations_fn, use_cache=use_cache))\n",
    "\n",
    "        activations = np.concatenate(all_activations, axis=1)\n",
    "        layers = self.load_data(\"activations\", layers_fn, use_cache=use_cache)\n",
    "        metadata_df = self.load_data(\"activations\", metadata_fn, use_cache=use_cache)\n",
    "        return activations, layers, metadata_df\n",
    "\n",
    "\n",
    "    def load_Xss(self, models, use_cache=True, compress_to_POS_6=False, v=1):\n",
    "        \"\"\"\n",
    "        Load activations for multiple models into the Xss attribute.\n",
    "\n",
    "        Args:\n",
    "            models (list or str): A list of model labels or a single model label to load.\n",
    "            use_cache (bool, optional): Whether to use cached data if available (default is True).\n",
    "            compress_to_POS_6 (bool, optional): Whether to compress the data to the POS-6 level (default is False).\n",
    "            v (int, optional): Verbosity level for printing information (default is 1).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of loaded activations.\n",
    "        \"\"\"\n",
    "        if not isinstance(models, list):\n",
    "            models = [models]\n",
    "\n",
    "        for model in tqdm(models, desc='loading models'):\n",
    "            if compress_to_POS_6:\n",
    "                if use_cache and model in list(self.Xss_6):\n",
    "                    continue  # skip if compressed model already loaded\n",
    "            else:\n",
    "                if use_cache and model in list(self.Xss): continue # skip if model already loaded\n",
    "            Xs={}\n",
    "            (activations, layers, metadata_df) = self.load_activations(model)\n",
    "            # sample_idx_min = metadata_df[\"context_length\"].idxmax()\n",
    "            # sample_idx_max = len(metadata_df[\"context_length\"])\n",
    "            # print(\"sample_idxes\",sample_idx_min,sample_idx_max)\n",
    "            for i in range(len(layers)):\n",
    "                Xs[layers[i]] = activations[i][self.sample_idx_min:self.sample_idx_max]\n",
    "                if compress_to_POS_6:\n",
    "                    Xs[layers[i]]=Xs[layers[i]].compress(self.is_in_POS_6,axis=0)\n",
    "                # Xs[layers[i]]=standardize_np_array(activations[i][sample_idx_min:sample_idx_max])\n",
    "            if compress_to_POS_6:\n",
    "                self.Xss_6[model]=Xs\n",
    "            else:\n",
    "                self.Xss[model]=Xs\n",
    "\n",
    "        Xss=self.Xss\n",
    "        Xss_6=self.Xss_6\n",
    "\n",
    "        if v:\n",
    "            if compress_to_POS_6:\n",
    "                print(f\"all models: {list(Xss_6.keys())}\")\n",
    "                print(\"=\"*4)\n",
    "                print(f\"{dFirst(dFirst(Xss_6)).shape[0]: <4} = number of samples\")\n",
    "                print(f\"{len(Xss_6): <4} = number of models\")\n",
    "                print(f\"{len(dFirst(Xss_6)): <4} = number of layers in 1st model\")\n",
    "                print(f\"Xss_6 shape: {dFirst(dFirst(Xss_6)).shape}\")\n",
    "                print(f\"Size of Xss_6: {str(get_size_of_dict(Xss_6))} GB\")\n",
    "                print(f\"Size of Xss: {str(get_size_of_dict(Xss))} GB\")\n",
    "                print(\"=\"*100)\n",
    "            else:\n",
    "                print(f\"all models: {list(Xss.keys())}\")\n",
    "                print(\"=\"*4)\n",
    "                print(f\"{dFirst(dFirst(Xss)).shape[0]: <4} = number of samples\")\n",
    "                print(f\"{len(Xss): <4} = number of models\")\n",
    "                print(f\"{len(dFirst(Xss)): <4} = number of layers in 1st model\")\n",
    "                print(f\"X shape: {dFirst(dFirst(Xss)).shape}\")\n",
    "                print(f\"Size of Xss: {str(get_size_of_dict(Xss))} GB\")\n",
    "                print(\"=\"*100)\n",
    "        if compress_to_POS_6:\n",
    "            return self.Xss_6\n",
    "        else:\n",
    "            return self.Xss\n",
    "\n",
    "\n",
    "    def load_ys(self, use_cache=True, compress_to_POS_6=False, v=1):\n",
    "        \"\"\"\n",
    "        Load target variables into the ys attribute, with optional compression to POS-6.\n",
    "\n",
    "        Args:\n",
    "            use_cache (bool, optional): Whether to use cached data if available (default is True).\n",
    "            compress_to_POS_6 (bool, optional): Whether to compress the data to the POS-6 level (default is False).\n",
    "            v (int, optional): Verbosity level for printing information (default is 1).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of loaded target variables.\n",
    "        \"\"\"\n",
    "        if use_cache and self.ys:\n",
    "            return\n",
    "        # sample_idx_min = self.sample_idx_min # 386\n",
    "        # sample_idx_max = self.sample_idx_max # 8344\n",
    "        # sentence_level_dict = load_sentence_level_dict()\n",
    "        # word_level_dict = load_word_level_dict()\n",
    "        word_features = self.load_dataset(\"word_features\")\n",
    "        # self.word_features = word_features\n",
    "        word_level_dict = word_features.to_dict(orient=\"list\")\n",
    "        mydatadict = {x: np.array(word_level_dict[x][self.sample_idx_min:self.sample_idx_max]) for x in tqdm(word_level_dict.keys(), desc='loading mydatadict')}\n",
    "\n",
    "        for x in self.classification_labels + self.reggression_labels + self.pos_labels:\n",
    "            if x[-1] == \"+\":\n",
    "                mydatadict[x] = np.roll(mydatadict[x[:-1]], -1)\n",
    "            if x[-1] == \"-\":\n",
    "                mydatadict[x] = np.roll(mydatadict[x[:-1]], 1)\n",
    "\n",
    "        self.is_in_POS_6 = mydatadict[\"is_in_POS_6\"]\n",
    "\n",
    "        for x in mydatadict.keys():\n",
    "            self.ys[x] = mydatadict[x]\n",
    "            if compress_to_POS_6:\n",
    "                self.ys_6[x]=self.ys[x].compress(self.is_in_POS_6,axis=0)\n",
    "\n",
    "        ys=self.ys\n",
    "        ys_6=self.ys_6\n",
    "\n",
    "        if v:\n",
    "            if compress_to_POS_6:\n",
    "                print(f\"{len(ys_6): <4} = number of possible targets\")\n",
    "                print(f\"y_6 shape: {dFirst(ys_6).shape}\")\n",
    "                print(f\"Size of ys_6: {str(get_size_of_dict(ys_6))} GB\")\n",
    "                print(f\"Size of ys: {str(get_size_of_dict(ys))} GB\")\n",
    "                print(\"=\"*100)\n",
    "                print(f\"\\nclassification_labels: \", end=\"\")\n",
    "                print_d({k:ys_6[k] for k in self.classification_labels})\n",
    "                print(f\"\\nreggression_labels: \", end=\"\")\n",
    "                print_d({k:ys_6[k] for k in self.reggression_labels})\n",
    "                print(f\"\\npos_labels: \", end=\"\")\n",
    "                print_d({k:ys_6[k] for k in self.pos_labels}); \n",
    "                print(f\"\\other_labels: \", end=\"\")\n",
    "                print_d({k:ys_6[k] for k in ys_6 if not k in self.pos_labels + self.reggression_labels + self.classification_labels})\n",
    "            else:\n",
    "                print(f\"{len(ys): <4} = number of possible targets\")\n",
    "                print(f\"y shape: {dFirst(ys).shape}\")\n",
    "                print(f\"Size of ys: {str(get_size_of_dict(ys))} GB\")\n",
    "                print(\"=\"*100)\n",
    "                print(f\"\\nclassification_labels: \", end=\"\")\n",
    "                print_d({k:ys[k] for k in self.classification_labels})\n",
    "                print(f\"\\nreggression_labels: \", end=\"\")\n",
    "                print_d({k:ys[k] for k in self.reggression_labels})\n",
    "                print(f\"\\npos_labels: \", end=\"\")\n",
    "                print_d({k:ys[k] for k in self.pos_labels}); \n",
    "                print(f\"\\other_labels: \", end=\"\")\n",
    "                print_d({k:ys[k] for k in ys if not k in self.pos_labels + self.reggression_labels + self.classification_labels})\n",
    "        if compress_to_POS_6:\n",
    "            return self.ys_6\n",
    "        else:\n",
    "            return self.ys\n",
    "\n",
    "\n",
    "    def standardize_Xss_ys(self):\n",
    "        \"\"\"\n",
    "        Standardize the activations and target variables for models and layers.\n",
    "\n",
    "        This function standardizes the values in `Xss` and `ys` attributes in place, using z-score normalization.\n",
    "        \"\"\"\n",
    "        for k, v in self.ys.items():\n",
    "            if k in self.reggression_labels:\n",
    "                self.ys[k] = standardize_np_array(v)\n",
    "            else:\n",
    "                self.ys[k] = v\n",
    "\n",
    "        for k, v in self.ys_6.items():\n",
    "            if k in self.reggression_labels:\n",
    "                self.ys_6[k] = standardize_np_array(v)\n",
    "            else:\n",
    "                self.ys_6[k] = v\n",
    "\n",
    "        for model, Xs in self.Xss.items():\n",
    "            for layer, X in Xs.items():\n",
    "                self.Xss[model][layer] = standardize_np_array(X)\n",
    "\n",
    "        for model, Xs in self.Xss_6.items():\n",
    "            for layer, X in Xs.items():\n",
    "                self.Xss_6[model][layer] = standardize_np_array(X)\n",
    "\n",
    "\n",
    "    def load_csvs_as_dataframe(self, dir_key, pattern=\"*.csv\"):\n",
    "        \"\"\"\n",
    "        Load all CSV files from a directory into a single pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            dir_key (str): The directory key.\n",
    "            pattern (str, optional): The file pattern to match (default is \"*.csv\").\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A concatenated DataFrame containing the data from all matching CSV files.\n",
    "        \"\"\"\n",
    "        files = self.get_files(dir_key, pattern)\n",
    "        dfs = [pd.read_csv(file, low_memory=False) for file in files]\n",
    "        return pd.concat(dfs, axis=1)\n",
    "\n",
    "\n",
    "    def load_all(self, dir_key):\n",
    "        \"\"\"\n",
    "        Load all files from a directory into the cache and return them as a list.\n",
    "\n",
    "        Args:\n",
    "            dir_key (str): The directory key.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of all loaded files.\n",
    "        \"\"\"\n",
    "        files = self.get_files(dir_key)\n",
    "        res = []\n",
    "        for file in files:\n",
    "            absolute_path = file.resolve()\n",
    "            if file.suffix == \".csv\":\n",
    "                data = pd.read_csv(file)\n",
    "            elif file.suffix == \".npy\":\n",
    "                data = np.load(file)\n",
    "            elif file.suffix == \".txt\":\n",
    "                with open(file, 'r') as f:\n",
    "                    data = f.read()\n",
    "            else:\n",
    "                print(f\"Warning: Unsupported file type {file.suffix} for {file}. Skipping.\")\n",
    "                continue\n",
    "            res.append(data)\n",
    "            self.file_cache[str(absolute_path)] = data\n",
    "        return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23fc1c5d-5a7c-4160-86f3-c017aa1ba9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME=os.getenv(\"HOME\")\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Local imports\n",
    "from my_utils import *\n",
    "\n",
    "\n",
    "### Example usage:\n",
    "# PM = ProjectManager()\n",
    "# data = PM.fetch_data_by_label(\"activations\", \"double_2\")\n",
    "\n",
    "### saving\n",
    "# PM.save_dataset(\"word_features\", PM.fn_word_frequencies, data=word_frequencies)\n",
    "# PM.get_files(\"transformer_weights\")\n",
    "\n",
    "### loading\n",
    "# sentence_features = PM.load_dataset(\"sentence_features\")\n",
    "# sentence_features\n",
    "# word_features = PM.load_dataset(\"word_features\")\n",
    "# word_features\n",
    "# gpt_input = PM.load_dataset(\"gpt_input\")\n",
    "# gpt_input\n",
    "\n",
    "class ProjectManager_old:\n",
    "    \"\"\"\n",
    "    Manages datasets, model activations, and project-specific file handling for language model experiments.\n",
    "\n",
    "    This class provides methods to load, save, and process data for various models, \n",
    "    including activations, word features, and sentence features. It handles caching \n",
    "    and offers tools to standardize and prepare data for analysis.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    Xss : dict\n",
    "        Stores activations for all models.\n",
    "    ys : dict\n",
    "        Stores target values for all classification and regression tasks.\n",
    "    directories : dict\n",
    "        Contains paths to various project directories (e.g., transformer weights, activations, scores).\n",
    "    model_label_map : dict\n",
    "        Maps model identifiers to display labels for easier interpretation.\n",
    "    layers_dict : dict\n",
    "        Maps model names to their respective layers.\n",
    "    \"\"\"\n",
    "    Xss={}\n",
    "    ys={}\n",
    "    ys_6={}\n",
    "    Xss_6={}\n",
    "    is_in_POS_6=[]\n",
    "    base_dir = Path(f\"{HOME}/data\")\n",
    "    directories = {\n",
    "        \"transformer_weights\": base_dir / \"transformer_weights\",\n",
    "        \"word_features\": base_dir / \"word_features\",\n",
    "        \"sentence_features\": base_dir / \"sentence_features\",\n",
    "        \"scores\": base_dir / \"scores\",\n",
    "        \"activations\": base_dir / \"activations\",\n",
    "        \"context_lengths\": base_dir / \"context_lengths\",\n",
    "        \"analysis_res\": base_dir / \"analysis_res\",\n",
    "        \"thesis_tex\": base_dir / \"thesis_tex\",\n",
    "        \"tbl\": base_dir / \"thesis_tex\" / \"tbl\",\n",
    "        \"fig\": base_dir / \"thesis_tex\" / \"fig\",\n",
    "        \"fimg\": base_dir / \"thesis_tex\" / \"fimg\",\n",
    "        \"hf_files\": base_dir / \"hf_files\",\n",
    "        \"gpt_input\": base_dir / \"gpt_input\",\n",
    "        \"exp1\": base_dir / \"experiment_1_results\",\n",
    "        \"exp2\": base_dir / \"experiment_2_results\",\n",
    "        \"exp3\": base_dir / \"experiment_3_results\",\n",
    "        \"exp3_test\": base_dir / \"experiment_3_test\",\n",
    "    }\n",
    "    fn_base_sent_features = \"base_sentence_features.csv\"\n",
    "    fn_base_word_features = \"base_word_features.csv\"\n",
    "    fn_word_indexes = \"word_indexes.csv\"\n",
    "    fn_pos_features = \"pos_features.csv\"\n",
    "    # fn_pos_names = \"pos_names.csv\"\n",
    "    fn_word_frequencies = \"word_frequencies.csv\"\n",
    "    fn_tree_depth = \"tree_depth.csv\"\n",
    "    fn_function = \"function.csv\"\n",
    "    \n",
    "    all_models = (\n",
    "        ['gpt2-xl','gpt2-xl-untrained_1','gpt2']+\n",
    "        [f\"gpt2-untrained_{i}\" for i in range(1,10)]+\n",
    "        [f\"gpt2-untrained_{i}_weight_config_all\" for i in range(1,10)]\n",
    "    )\n",
    "    gpt2xl_models=all_models[:2]\n",
    "    gpt2_models=all_models[2:]\n",
    "    # Mapping between model labels and their display labels\n",
    "    layers_gpt2xl=[\"drop\"]+[f'encoder.h.{i}' for i in range(48)]\n",
    "    layers_gpt2=layers_gpt2xl[:13]\n",
    "    layers_dict={}\n",
    "    layers_to_idx={}\n",
    "    model_label_map = {\n",
    "        'gpt2-xl': 'XL-Trained',\n",
    "        'gpt2-xl-untrained_1': 'XL-Untrained_1',\n",
    "        'gpt2': 'Trained',\n",
    "        'gpt2-untrained_1_weight_config_all': 'Guassian_1',\n",
    "        'gpt2-untrained_2_weight_config_all': 'Guassian_2',\n",
    "        'gpt2-untrained_3_weight_config_all': 'Guassian_3',\n",
    "        'gpt2-untrained_4_weight_config_all': 'Guassian_4',\n",
    "        'gpt2-untrained_5_weight_config_all': 'Guassian_5',\n",
    "        'gpt2-untrained_6_weight_config_all': 'Guassian_6',\n",
    "        'gpt2-untrained_7_weight_config_all': 'Guassian_7',\n",
    "        'gpt2-untrained_8_weight_config_all': 'Guassian_8',\n",
    "        'gpt2-untrained_9_weight_config_all': 'Guassian_9',\n",
    "        'gpt2-untrained_1': 'Untrained_1',\n",
    "        'gpt2-untrained_2': 'Untrained_2',\n",
    "        'gpt2-untrained_3': 'Untrained_3',\n",
    "        'gpt2-untrained_4': 'Untrained_4',\n",
    "        'gpt2-untrained_5': 'Untrained_5',\n",
    "        'gpt2-untrained_6': 'Untrained_6',\n",
    "        'gpt2-untrained_7': 'Untrained_7',\n",
    "        'gpt2-untrained_8': 'Untrained_8',\n",
    "        'gpt2-untrained_9': 'Untrained_9',}\n",
    "    \n",
    "    # Define ordered groups of models\n",
    "    model_group_map={'gpt2-xl': 'XL-Trained',\n",
    "        'gpt2-xl-untrained_1': 'XL-Untrained',\n",
    "        'gpt2': 'Trained',\n",
    "        'gpt2-untrained_1': 'Untrained',\n",
    "        'gpt2-untrained_2': 'Untrained',\n",
    "        'gpt2-untrained_3': 'Untrained',\n",
    "        'gpt2-untrained_4': 'Untrained',\n",
    "        'gpt2-untrained_5': 'Untrained',\n",
    "        'gpt2-untrained_6': 'Untrained',\n",
    "        'gpt2-untrained_7': 'Untrained',\n",
    "        'gpt2-untrained_8': 'Untrained',\n",
    "        'gpt2-untrained_9': 'Untrained',\n",
    "        'gpt2-untrained_1_weight_config_all': 'Guassian',\n",
    "        'gpt2-untrained_2_weight_config_all': 'Guassian',\n",
    "        'gpt2-untrained_3_weight_config_all': 'Guassian',\n",
    "        'gpt2-untrained_4_weight_config_all': 'Guassian',\n",
    "        'gpt2-untrained_5_weight_config_all': 'Guassian',\n",
    "        'gpt2-untrained_6_weight_config_all': 'Guassian',\n",
    "        'gpt2-untrained_7_weight_config_all': 'Guassian',\n",
    "        'gpt2-untrained_8_weight_config_all': 'Guassian',\n",
    "        'gpt2-untrained_9_weight_config_all': 'Guassian',}\n",
    "    model_groups = {\n",
    "        \"main\": [\"gpt2\"]+[f\"gpt2-untrained_{i}\" for i in range(10)]+[\"all\"],\n",
    "        \"single\": [f\"single_{i}\" for i in range(12)],\n",
    "        \"doubles\": [f\"double_{i}\" for i in range(6)],\n",
    "        \"quads\":['attns','mlps','lns'],\n",
    "    }\n",
    "\n",
    "    # Part of Speech tags\n",
    "    # pos documentation: https://data.mendeley.com/datasets/zmycy7t9h9/2\n",
    "    # where XX is for pos tag missing, and -LRB-/-RRB- is \"(\" / \")\".\n",
    "    POS_51_all_tags=[\"XX\", \"``\", \"$\", \"''\", \"*\", \",\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"ADD\", \"AFX\", \"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"HYPH\", \"IN\",\n",
    "           \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NFP\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\",\n",
    "           \"SYM\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"VERB\", \"WDT\", \"WP\", \"WP$\", \"WRB\"] \n",
    "    POS_12_all_tags=[\"VERB\",\"NOUN\",\"PRON\",\"ADJ\",\"ADV\",\"ADP\",\"CONJ\",\"DET\",\"NUM\",\"PRT\",\"X\",\".\"]\n",
    "    POS_7_all_tags=[\"Noun\",\"Verb\",\"Adposition\",\"Determiner\",\"Adjective\",\"Adverb\",\"X\"]\n",
    "    POS_6_all_tags=[\"Noun\",\"Verb\",\"Adposition\",\"Determiner\",\"Adjective\",\"Adverb\"]\n",
    "    POS_12_to_POS_7={\n",
    "        \"NOUN\":\"Noun\",\n",
    "        \"PRON\":\"Noun\",\n",
    "        \"VERB\":\"Verb\",\n",
    "        \"ADP\":\"Adposition\",\n",
    "        \"DET\":\"Determiner\",\n",
    "        \"ADJ\":\"Adjective\",\n",
    "        \"ADV\":\"Adverb\",\n",
    "        \"CONJ\":\"X\",\n",
    "        \"NUM\":\"X\",\n",
    "        \"PRT\":\"X\",\n",
    "        \"X\":\"X\",\n",
    "        \".\":\"X\"}\n",
    "    POS_51_tag_to_id={x:i for i,x in enumerate(POS_51_all_tags)}\n",
    "    POS_12_tag_to_id={x:i for i,x in enumerate(POS_12_all_tags)}\n",
    "    POS_7_tag_to_id={x:i for i,x in enumerate(POS_7_all_tags)}\n",
    "    POS_6_tag_to_id=POS_7_tag_to_id\n",
    "\n",
    "    pos_names=[\"XX\", \"``\", \"$\", \"''\", \"*\", \",\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"ADD\", \"AFX\", \"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"HYPH\", \"IN\",\n",
    "           \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NFP\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\",\n",
    "           \"SYM\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"VERB\", \"WDT\", \"WP\", \"WP$\", \"WRB\",]\n",
    "    sample_idx_min=386\n",
    "    sample_idx_max=8344\n",
    "    classification_labels = []\n",
    "    for x in ['words','word_idx','predicate_lemmas', 'predicate_framenet_ids', 'word_senses', 'named_entities', 'function', 'tree_depth']:\n",
    "        classification_labels+=[x, x + \"-\", x + \"+\"]\n",
    "    my_classification_targets = []\n",
    "    for x in ['function', 'tree_depth','word_idx']:\n",
    "        my_classification_targets+=[x, x + \"-\", x + \"+\"]\n",
    "    reggression_labels = [] # reggression_labels=['sentence_idx','unigram_probs', 'unigram_probs-', 'unigram_probs+', 'bigram_probs', 'bigram_probs-', 'bigram_probs+', 'trigram_probs','trigram_probs-', 'trigram_probs+']\n",
    "    for x in ['sentence_idx','unigram_probs', 'bigram_probs', 'trigram_probs']:\n",
    "        reggression_labels+=[x, x + \"-\", x + \"+\"]\n",
    "    pos_labels = [] # pos_labels=['pos_tags','pos_tags-', 'pos_tags+']\n",
    "    for x in ['pos_tags', 'POS_12_id', 'POS_7_id']:\n",
    "        pos_labels+=[x, x + \"-\", x + \"+\"]\n",
    "\n",
    "    sels=[f\"sel_{i}\" for i in range(6)]\n",
    "    maxps=[f\"maxp_{i}\" for i in range(6)]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.file_cache = {}\n",
    "        self.layers_dict={\n",
    "            **{x:self.layers_gpt2xl for x in self.gpt2xl_models},\n",
    "            **{x:self.layers_gpt2 for x in self.gpt2_models}\n",
    "            }\n",
    "        self.layers_to_idx={x:i for i,x in enumerate(self.layers_gpt2xl)}\n",
    "        \n",
    "    def _get_model_label(self, display_label):\n",
    "        for model_label, disp_label in self.model_label_map.items():\n",
    "            if disp_label == display_label or model_label == display_label:\n",
    "                return model_label\n",
    "        raise ValueError(f\"Unknown display label: {display_label}\")\n",
    "    def _get_display_label(self, model_label):\n",
    "        for _model_label, display_label in self.model_label_map.items():\n",
    "            if model_label == _model_label or model_label == display_label:\n",
    "                return display_label\n",
    "        raise ValueError(f\"Unknown display label: {display_label}\")\n",
    "\n",
    "    def get_base_and_full_labels(self, model_label):\n",
    "        if model_label in [\"gpt2\",\"gpt2-untrained\", \"gpt2-xl\",\"gpt2-xl-untrained\"]:\n",
    "            base_label=model_label\n",
    "            full_label=model_label\n",
    "        elif \"_weight_config_\" in model_label:\n",
    "            base_label=\"gpt2-untrained\"\n",
    "            full_label = f\"{model_label}\"\n",
    "        else:\n",
    "            base_label=\"gpt2-untrained\"\n",
    "            full_label = f\"{base_label}_weight_config_{model_label}\"\n",
    "        return base_label, full_label\n",
    "\n",
    "    def check_for_existing_patterns(self, dir_key):\n",
    "        return [x for x in self.all_models if self.do_patterns_exist(dir_key, x)]\n",
    "\n",
    "    def do_patterns_exist(self, dir_key, model_label):\n",
    "        files=self.generate_file_patterns(dir_key, model_label)\n",
    "        # print(dir_key, model_label,files)\n",
    "        if isinstance(files,tuple):\n",
    "            files=list(files)\n",
    "        else:\n",
    "            files=[files]\n",
    "        return all(Path(self.directories[dir_key],f).is_file() for f in files)\n",
    "\n",
    "    def generate_file_patterns(self, dir_key, model_label):\n",
    "        \"\"\"\n",
    "        Generate file patterns for a given model label and directory key.\n",
    "        \"\"\"\n",
    "        base_label, full_label = self.get_base_and_full_labels(model_label)\n",
    "        if dir_key == \"activations\":\n",
    "            return (\n",
    "                f\"{full_label}__activations_v1_i0.npy\",\n",
    "                f\"{full_label}__activations_v1_i0_metadata.csv\",\n",
    "                f\"{full_label}_layers.txt\"\n",
    "            )\n",
    "        elif dir_key == \"scores\":\n",
    "            # The scores seem to have a timestamp, so we use a wildcard for that part\n",
    "            return f\"{full_label}_*score_raw.csv\"\n",
    "        elif dir_key == \"word_features\" or dir_key == \"sentence_features\":\n",
    "            # Assuming a generic pattern for word and sentence features; adjust as needed\n",
    "            return f\"{full_label}_*.csv\"\n",
    "        elif dir_key == \"transformer_saved_models\":\n",
    "            return f\"E1_Schrimpfs_{base_label}\"\n",
    "        elif dir_key == \"transformer_weights\":\n",
    "            return f\"{full_label}.pt\"\n",
    "        # Add more dir_key conditions as needed\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported directory key for pattern generation: {dir_key}\")\n",
    "\n",
    "    def fetch_data_by_label(self, dir_key, display_label):\n",
    "        model_label = self._get_model_label(display_label)\n",
    "        base_label, full_label = self.get_base_and_full_labels(model_label)\n",
    "\n",
    "        if dir_key == \"activations\":\n",
    "            pattern = f\"{full_label}__activations*.*\"\n",
    "        elif dir_key == \"scores\":\n",
    "            pattern = f\"{full_label}_*score_raw.csv\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported directory key for fetching: {dir_key}\")\n",
    "\n",
    "        files = self.get_files(dir_key, pattern)\n",
    "\n",
    "        if dir_key == \"activations\":\n",
    "            data = [np.load(file) for file in files if file.suffix == \".npy\"]\n",
    "        elif dir_key == \"scores\":\n",
    "            data = [pd.read_csv(file) for file in files]\n",
    "        else:\n",
    "            data = []\n",
    "        return data\n",
    "\n",
    "    def get_files(self, dir_key, pattern=\"*\"):\n",
    "        \"\"\"\n",
    "        Get all files in a directory matching a pattern.\n",
    "        \"\"\"\n",
    "        dir_path = self.directories.get(dir_key)\n",
    "        if not dir_path:\n",
    "            raise ValueError(f\"Unknown directory key: {dir_key}\")\n",
    "        return list(dir_path.glob(pattern))\n",
    "\n",
    "\n",
    "    def check_if_data_exists(self, dir_key, file_name):\n",
    "        \"\"\"\n",
    "        Load data from a file. Cache it for faster subsequent loads.\n",
    "        \"\"\"\n",
    "        file_path = self.directories[dir_key] / file_name\n",
    "        return file_path.exists()\n",
    "\n",
    "    def save_data(self, dir_key, file_name, data, use_cache=True): # PM.save_data(dir_key, file_name, data)\n",
    "        \"\"\"\n",
    "        Save data to a file.\n",
    "        \"\"\"\n",
    "        file_path = self.directories[dir_key] / file_name\n",
    "        # print(f\"saving data to file_path: {file_path}, with data of type: {type(data)}\")\n",
    "        if file_path.suffix == \".csv\":\n",
    "            if isinstance(data, dict):\n",
    "                pd.DataFrame.from_dict(data).to_csv(file_path, index=False)\n",
    "            elif isinstance(data,pd.DataFrame):\n",
    "                data.to_csv(file_path, index=False)\n",
    "            else:\n",
    "                raise ValueError(f\"tried to save csv with an unsuppported data type: {type(data)}\")\n",
    "        elif file_path.suffix == \".npy\":\n",
    "            np.save(file_path, data)\n",
    "        elif file_path.suffix == \".txt\" and isinstance(data,list):\n",
    "            with open(file_path, 'w') as f:\n",
    "                for x in data:\n",
    "                    f.write(x + '\\n')\n",
    "        elif file_path.suffix == \".txt\":\n",
    "            with open(file_path, 'w') as f:\n",
    "                f.write(data)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_path.suffix}\")\n",
    "\n",
    "        # Update cache\n",
    "        if use_cache:\n",
    "            self.file_cache[file_name] = data\n",
    "\n",
    "\n",
    "    def load_data(self, dir_key, file_name, use_cache=False):\n",
    "        \"\"\"\n",
    "        Load data from a file. Cache it for faster subsequent loads.\n",
    "        \"\"\"\n",
    "        file_path = self.directories[dir_key] / file_name\n",
    "        \n",
    "        if use_cache and file_name in self.file_cache:\n",
    "            return self.file_cache[file_name]\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            return None\n",
    "        \n",
    "        if file_path.suffix == \".csv\":\n",
    "            data = pd.read_csv(file_path)\n",
    "        elif file_path.suffix == \".npy\":\n",
    "            data = np.load(file_path)\n",
    "        elif file_path.suffix == \".txt\" and \"layers\" in file_name:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = [line.strip() for line in f]\n",
    "        elif file_path.suffix == \".txt\":\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = f.read()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_path.suffix}\")\n",
    "            \n",
    "        if use_cache:\n",
    "            self.file_cache[file_name] = data\n",
    "        return data\n",
    "\n",
    "    def save_dataset(self, dir_key, custom_name=None, data=None):\n",
    "        \"\"\"\n",
    "        Save non-model-specific datasets using standardized naming conventions.\n",
    "        \"\"\"\n",
    "        if dir_key not in [\"word_features\", \"sentence_features\", \"gpt_input\"]:\n",
    "            raise ValueError(f\"Unsupported directory key for saving dataset: {dir_key}\")\n",
    "\n",
    "        # Use custom name if provided, else use a standardized name\n",
    "        file_name = custom_name if custom_name else f\"{dir_key}_dataset.csv\"\n",
    "        self.save_data(dir_key, file_name, data)\n",
    "\n",
    "    def load_dataset(self, dir_key, custom_name=None):\n",
    "        \"\"\"\n",
    "        Load non-model-specific datasets using standardized naming conventions.\n",
    "        \"\"\"\n",
    "        if dir_key not in [\"word_features\", \"sentence_features\", \"gpt_input\"]:\n",
    "            raise ValueError(f\"Unsupported directory key for loading dataset: {dir_key}\")\n",
    "\n",
    "        # Use custom name if provided, else use a standardized name\n",
    "        if custom_name:\n",
    "            return self.load_data(dir_key, custom_name)\n",
    "        else:\n",
    "            return self.load_csvs_as_dataframe(dir_key)\n",
    "\n",
    "    def save_activations(self, activations, layers, model, metadata_df, use_cache=False): # PM.save_activations(activations, layers, model, metadata_df)\n",
    "        # label, activations_fn, metadata_fn, layers_fn  = get_activation_names(model, version, activations_index)\n",
    "        activations_fn, metadata_fn, layers_fn = self.generate_file_patterns(\"activations\", model)\n",
    "        PM.save_data(\"activations\", activations_fn, activations,use_cache=use_cache)\n",
    "        PM.save_data(\"activations\", layers_fn, layers,use_cache=use_cache)\n",
    "        PM.save_data(\"activations\", metadata_fn, metadata_df,use_cache=use_cache)\n",
    "\n",
    "    def load_activations(self, model, version=1, activations_indexes=[0], use_cache=False): # PM.load_activations(model)\n",
    "        all_activations=[]\n",
    "        for activations_index in activations_indexes:\n",
    "            activations_fn, metadata_fn, layers_fn = self.generate_file_patterns(\"activations\", model)\n",
    "            # label, activations_fn, layers_fn, metadata_fn = get_activation_names(model, version, activations_index)\n",
    "            all_activations.append(self.load_data(\"activations\",activations_fn,use_cache=use_cache))\n",
    "        activations=np.concatenate(all_activations,axis=1)\n",
    "        # load layers\n",
    "        layers=self.load_data(\"activations\",layers_fn,use_cache=use_cache)\n",
    "        metadata_df = self.load_data(\"activations\", metadata_fn,use_cache=use_cache)\n",
    "        return activations, layers, metadata_df\n",
    "\n",
    "\n",
    "    # # sample_limit=10000\n",
    "    # # if sample_limit and sample_limit < PM.sample_idx_max - PM.sample_idx_min: PM.sample_idx_max = PM.sample_idx_min + sample_limit\n",
    "    # # PM.load_ys() # , use_cache=False)\n",
    "    # # PM.load_Xss(all_models) # , use_cache=False)\n",
    "    # ys_6 = PM.load_ys(compress_to_POS_6=True,v=0) # , use_cache=False)\n",
    "    # Xss_6 = PM.load_Xss(all_models, compress_to_POS_6=True,v=0) # , use_cache=False)\n",
    "    # if False: PM.standardize_Xss_ys()\n",
    "    def load_Xss(self, models, use_cache=True, compress_to_POS_6=False,v=1):\n",
    "\n",
    "        if not isinstance(models,list): models=[models]\n",
    "        for model in tqdm(models, desc='loading models'):\n",
    "            if compress_to_POS_6:\n",
    "                if use_cache and model in list(self.Xss_6): continue  # skip if compressed model already loaded\n",
    "            else:\n",
    "                if use_cache and model in list(self.Xss): continue # skip if model already loaded\n",
    "            Xs={}\n",
    "            (activations, layers, metadata_df) = self.load_activations(model)\n",
    "            # sample_idx_min = metadata_df[\"context_length\"].idxmax()\n",
    "            # sample_idx_max = len(metadata_df[\"context_length\"])\n",
    "            # print(\"sample_idxes\",sample_idx_min,sample_idx_max)\n",
    "            for i in range(len(layers)):\n",
    "                Xs[layers[i]]=activations[i][self.sample_idx_min:self.sample_idx_max]\n",
    "                if compress_to_POS_6:\n",
    "                    Xs[layers[i]]=Xs[layers[i]].compress(self.is_in_POS_6,axis=0)\n",
    "                # Xs[layers[i]]=standardize_np_array(activations[i][sample_idx_min:sample_idx_max])\n",
    "            if compress_to_POS_6:\n",
    "                self.Xss_6[model]=Xs\n",
    "            else:\n",
    "                self.Xss[model]=Xs\n",
    "\n",
    "        Xss=self.Xss\n",
    "        Xss_6=self.Xss_6\n",
    "\n",
    "        if v:\n",
    "            if compress_to_POS_6:\n",
    "                print(f\"all models: {list(Xss_6.keys())}\")\n",
    "                print(\"=\"*4)\n",
    "                print(f\"{dFirst(dFirst(Xss_6)).shape[0]: <4} = number of samples\")\n",
    "                print(f\"{len(Xss_6): <4} = number of models\")\n",
    "                print(f\"{len(dFirst(Xss_6)): <4} = number of layers in 1st model\")\n",
    "                print(f\"Xss_6 shape: {dFirst(dFirst(Xss_6)).shape}\")\n",
    "                print(f\"Size of Xss_6: {str(get_size_of_dict(Xss_6))} GB\")\n",
    "                print(f\"Size of Xss: {str(get_size_of_dict(Xss))} GB\")\n",
    "                print(\"=\"*100)\n",
    "            else:\n",
    "                print(f\"all models: {list(Xss.keys())}\")\n",
    "                print(\"=\"*4)\n",
    "                print(f\"{dFirst(dFirst(Xss)).shape[0]: <4} = number of samples\")\n",
    "                print(f\"{len(Xss): <4} = number of models\")\n",
    "                print(f\"{len(dFirst(Xss)): <4} = number of layers in 1st model\")\n",
    "                print(f\"X shape: {dFirst(dFirst(Xss)).shape}\")\n",
    "                print(f\"Size of Xss: {str(get_size_of_dict(Xss))} GB\")\n",
    "                print(\"=\"*100)\n",
    "\n",
    "        if compress_to_POS_6:\n",
    "            return self.Xss_6\n",
    "        else:\n",
    "            return self.Xss\n",
    "\n",
    "\n",
    "    def load_ys(self, use_cache=True, compress_to_POS_6=False,v=1):\n",
    "        if use_cache and self.ys:\n",
    "            return\n",
    "        # sample_idx_min = self.sample_idx_min # 386\n",
    "        # sample_idx_max = self.sample_idx_max # 8344\n",
    "        # sentence_level_dict = load_sentence_level_dict()\n",
    "        # word_level_dict = load_word_level_dict()\n",
    "        word_features = self.load_dataset(\"word_features\")\n",
    "        # self.word_features = word_features\n",
    "        word_level_dict = word_features.to_dict(orient=\"list\")\n",
    "        mydatadict={x: np.array(word_level_dict[x][self.sample_idx_min:self.sample_idx_max]) for x in tqdm(word_level_dict.keys(), desc='loading mydatadict')}\n",
    "        # mydataset=load_from_disk('mydata_with_index.hf')\n",
    "        # deal with rolled data labels\n",
    "        for x in self.classification_labels + self.reggression_labels + self.pos_labels:\n",
    "            if x[-1] == \"+\": mydatadict[x] = np.roll(mydatadict[x[:-1]], -1)\n",
    "            if x[-1] == \"-\": mydatadict[x] = np.roll(mydatadict[x[:-1]], 1)\n",
    "\n",
    "        # self.ys = dict([(x, mydatadict[x]) for x in self.classification_labels + self.reggression_labels + self.pos_labels]) # ys_reggression, ys_classification\n",
    "        self.is_in_POS_6 = mydatadict[\"is_in_POS_6\"]\n",
    "        # for x in self.classification_labels + self.reggression_labels + self.pos_labels:\n",
    "        for x in mydatadict.keys():\n",
    "            self.ys[x] = mydatadict[x]\n",
    "            if compress_to_POS_6:\n",
    "                self.ys_6[x]=self.ys[x].compress(self.is_in_POS_6,axis=0)\n",
    "\n",
    "        ys=self.ys\n",
    "        ys_6=self.ys_6\n",
    "\n",
    "        if v:\n",
    "            if compress_to_POS_6:\n",
    "                print(f\"{len(ys_6): <4} = number of possible targets\")\n",
    "                print(f\"y_6 shape: {dFirst(ys_6).shape}\")\n",
    "                print(f\"Size of ys_6: {str(get_size_of_dict(ys_6))} GB\")\n",
    "                print(f\"Size of ys: {str(get_size_of_dict(ys))} GB\")\n",
    "                print(\"=\"*100)\n",
    "                print(f\"\\nclassification_labels: \", end=\"\")\n",
    "                print_d({k:ys_6[k] for k in self.classification_labels})\n",
    "                print(f\"\\nreggression_labels: \", end=\"\")\n",
    "                print_d({k:ys_6[k] for k in self.reggression_labels})\n",
    "                print(f\"\\npos_labels: \", end=\"\")\n",
    "                print_d({k:ys_6[k] for k in self.pos_labels}); \n",
    "                print(f\"\\other_labels: \", end=\"\")\n",
    "                print_d({k:ys_6[k] for k in ys_6 if not k in self.pos_labels + self.reggression_labels + self.classification_labels})\n",
    "            else:\n",
    "                print(f\"{len(ys): <4} = number of possible targets\")\n",
    "                print(f\"y shape: {dFirst(ys).shape}\")\n",
    "                print(f\"Size of ys: {str(get_size_of_dict(ys))} GB\")\n",
    "                print(\"=\"*100)\n",
    "                print(f\"\\nclassification_labels: \", end=\"\")\n",
    "                print_d({k:ys[k] for k in self.classification_labels})\n",
    "                print(f\"\\nreggression_labels: \", end=\"\")\n",
    "                print_d({k:ys[k] for k in self.reggression_labels})\n",
    "                print(f\"\\npos_labels: \", end=\"\")\n",
    "                print_d({k:ys[k] for k in self.pos_labels}); \n",
    "                print(f\"\\other_labels: \", end=\"\")\n",
    "                print_d({k:ys[k] for k in ys if not k in self.pos_labels + self.reggression_labels + self.classification_labels})\n",
    "\n",
    "        if compress_to_POS_6:\n",
    "            return self.ys_6\n",
    "        else:\n",
    "            return self.ys\n",
    "\n",
    "    def standardize_Xss_ys(self):\n",
    "        for k,v in self.ys.items():\n",
    "            if k in self.reggression_labels:\n",
    "                self.ys[k]=standardize_np_array(v)\n",
    "            else:\n",
    "                self.ys[k]=v\n",
    "        for k,v in self.ys_6.items():\n",
    "            if k in self.reggression_labels:\n",
    "                self.ys_6[k]=standardize_np_array(v)\n",
    "            else:\n",
    "                self.ys_6[k]=v\n",
    "        for model,Xs in self.Xss.items():\n",
    "            for layer, X in Xs.items():\n",
    "                self.Xss[model][layer]=standardize_np_array(X)\n",
    "        for model,Xs in self.Xss_6.items():\n",
    "            for layer, X in Xs.items():\n",
    "                self.Xss_6[model][layer]=standardize_np_array(X)\n",
    "\n",
    "    # def save_analysis(self, model, version=1, activations_indexes=[0]): # PM.load_activations(model)\n",
    "    # def load_analysis(self, model, version=1, activations_indexes=[0]): # PM.load_activations(model)\n",
    "\n",
    "    def load_csvs_as_dataframe(self, dir_key, pattern=\"*.csv\"):\n",
    "        \"\"\"\n",
    "        Load all CSVs in a directory into a single dataframe.\n",
    "        \"\"\"\n",
    "        files = self.get_files(dir_key, pattern)\n",
    "        dfs = [pd.read_csv(file, low_memory=False) for file in files]\n",
    "        # return pd.concat(dfs, ignore_index=True)\n",
    "        return pd.concat(dfs, axis=1)\n",
    "\n",
    "    def load_all(self, dir_key):\n",
    "        \"\"\"\n",
    "        Load all files from a specified directory into the cache.\n",
    "        \"\"\"\n",
    "        files = self.get_files(dir_key)\n",
    "        res=[]\n",
    "        for file in files:\n",
    "            # Use the absolute path as the key for cache for uniqueness\n",
    "            absolute_path = file.resolve()\n",
    "            if file.suffix == \".csv\":\n",
    "                data = pd.read_csv(file)\n",
    "            elif file.suffix == \".npy\":\n",
    "                data = np.load(file)\n",
    "            elif file.suffix == \".txt\":\n",
    "                with open(file, 'r') as f:\n",
    "                    data = f.read()\n",
    "            else:\n",
    "                print(f\"Warning: Unsupported file type {file.suffix} for {file}. Skipping.\")\n",
    "                continue\n",
    "            res.append(data)\n",
    "            self.file_cache[str(absolute_path)] = data\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe489961-414a-452a-a423-bb57c648ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(vars(PM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3babd5b1-6e7b-4db2-8909-9d9d5f3c93e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_initialize_analysis_labels',\n",
       " '_initialize_model_mappings',\n",
       " '_initialize_pos_tags'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'x'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['POS_12_all_tags',\n",
       " 'POS_12_tag_to_id',\n",
       " 'POS_12_to_POS_7',\n",
       " 'POS_51_all_tags',\n",
       " 'POS_51_tag_to_id',\n",
       " 'POS_6_all_tags',\n",
       " 'POS_6_tag_to_id',\n",
       " 'POS_7_all_tags',\n",
       " 'POS_7_tag_to_id',\n",
       " 'Xss',\n",
       " 'Xss_6',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_get_display_label',\n",
       " '_get_model_label',\n",
       " 'all_models',\n",
       " 'base_dir',\n",
       " 'check_for_existing_patterns',\n",
       " 'check_if_data_exists',\n",
       " 'classification_labels',\n",
       " 'directories',\n",
       " 'do_patterns_exist',\n",
       " 'fetch_data_by_label',\n",
       " 'file_cache',\n",
       " 'fn_base_sent_features',\n",
       " 'fn_base_word_features',\n",
       " 'fn_function',\n",
       " 'fn_pos_features',\n",
       " 'fn_tree_depth',\n",
       " 'fn_word_frequencies',\n",
       " 'fn_word_indexes',\n",
       " 'generate_file_patterns',\n",
       " 'get_base_and_full_labels',\n",
       " 'get_files',\n",
       " 'gpt2_models',\n",
       " 'gpt2xl_models',\n",
       " 'is_in_POS_6',\n",
       " 'layers_dict',\n",
       " 'layers_gpt2',\n",
       " 'layers_gpt2xl',\n",
       " 'layers_to_idx',\n",
       " 'load_Xss',\n",
       " 'load_activations',\n",
       " 'load_all',\n",
       " 'load_csvs_as_dataframe',\n",
       " 'load_data',\n",
       " 'load_dataset',\n",
       " 'load_ys',\n",
       " 'maxps',\n",
       " 'model_group_map',\n",
       " 'model_groups',\n",
       " 'model_label_map',\n",
       " 'my_classification_targets',\n",
       " 'pos_labels',\n",
       " 'pos_names',\n",
       " 'reggression_labels',\n",
       " 'sample_idx_max',\n",
       " 'sample_idx_min',\n",
       " 'save_activations',\n",
       " 'save_data',\n",
       " 'save_dataset',\n",
       " 'sels',\n",
       " 'standardize_Xss_ys',\n",
       " 'x',\n",
       " 'ys',\n",
       " 'ys_6']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PM = ProjectManager()  # Instantiate the ProjectManager\n",
    "PM_old = ProjectManager_old()  # Instantiate the ProjectManager\n",
    "# dir(PM)\n",
    "temp1 = set(dir(PM)) - set(dir(PM_old))\n",
    "temp2 = set(dir(PM_old)) - set(dir(PM))\n",
    "temp1\n",
    "temp2\n",
    "dir(PM_old)\n",
    "# list(vars(PM_old))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "843667e0-49c3-4d70-8dc7-39fa1d1d2c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer_weights': PosixPath('/home/ben/data/transformer_weights'),\n",
       " 'word_features': PosixPath('/home/ben/data/word_features'),\n",
       " 'sentence_features': PosixPath('/home/ben/data/sentence_features'),\n",
       " 'scores': PosixPath('/home/ben/data/scores'),\n",
       " 'activations': PosixPath('/home/ben/data/activations'),\n",
       " 'context_lengths': PosixPath('/home/ben/data/context_lengths'),\n",
       " 'analysis_res': PosixPath('/home/ben/data/analysis_res'),\n",
       " 'thesis_tex': PosixPath('/home/ben/data/thesis_tex'),\n",
       " 'tbl': PosixPath('/home/ben/data/thesis_tex/tbl'),\n",
       " 'fig': PosixPath('/home/ben/data/thesis_tex/fig'),\n",
       " 'fimg': PosixPath('/home/ben/data/thesis_tex/fimg'),\n",
       " 'hf_files': PosixPath('/home/ben/data/hf_files'),\n",
       " 'gpt_input': PosixPath('/home/ben/data/gpt_input'),\n",
       " 'exp1': PosixPath('/home/ben/data/experiment_1_results'),\n",
       " 'exp2': PosixPath('/home/ben/data/experiment_2_results'),\n",
       " 'exp3': PosixPath('/home/ben/data/experiment_3_results'),\n",
       " 'exp3_test': PosixPath('/home/ben/data/experiment_3_test')}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d7e59-c526-43a6-8c5e-1126ac79c465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ea492-903b-4e10-bbdb-aba2a6670280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18963ac-3408-438e-9b68-22eb2ef9a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e83c04-7147-4230-9796-00dd6f6dc323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1eecc-c870-4c84-9e70-b47347420a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99c4e33c-dc0d-4d1b-8278-4f21033a9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "latexAccents = {\n",
    "    u\"à\": \"\\\\`a \", u\"è\": \"\\\\`e \", u\"ì\": \"\\\\`\\\\i \", u\"ò\": \"\\\\`o \", u\"ù\": \"\\\\`u \", u\"ỳ\": \"\\\\`y \",\n",
    "    u\"À\": \"\\\\`A \", u\"È\": \"\\\\`E \", u\"Ì\": \"\\\\`\\\\I \", u\"Ò\": \"\\\\`O \", u\"Ù\": \"\\\\`U \", u\"Ỳ\": \"\\\\`Y \",\n",
    "    u\"á\": \"\\\\'a \", u\"é\": \"\\\\'e \", u\"í\": \"\\\\'\\\\i \", u\"ó\": \"\\\\'o \", u\"ú\": \"\\\\'u \", u\"ý\": \"\\\\'y \",\n",
    "    u\"Á\": \"\\\\'A \", u\"É\": \"\\\\'E \", u\"Í\": \"\\\\'\\\\I \", u\"Ó\": \"\\\\'O \", u\"Ú\": \"\\\\'U \", u\"Ý\": \"\\\\'Y \",\n",
    "    u\"â\": \"\\\\^a \", u\"ê\": \"\\\\^e \", u\"î\": \"\\\\^\\\\i \", u\"ô\": \"\\\\^o \", u\"û\": \"\\\\^u \", u\"ŷ\": \"\\\\^y \",\n",
    "    u\"Â\": \"\\\\^A \", u\"Ê\": \"\\\\^E \", u\"Î\": \"\\\\^\\\\I \", u\"Ô\": \"\\\\^O \", u\"Û\": \"\\\\^U \", u\"Ŷ\": \"\\\\^Y \",\n",
    "    u\"ä\": \"\\\\\\\"a \", u\"ë\": \"\\\\\\\"e \", u\"ï\": \"\\\\\\\"\\\\i \", u\"ö\": \"\\\\\\\"o \", u\"ü\": \"\\\\\\\"u \", u\"ÿ\": \"\\\\\\\"y \",\n",
    "    u\"Ä\": \"\\\\\\\"A \", u\"Ë\": \"\\\\\\\"E \", u\"Ï\": \"\\\\\\\"\\\\I \", u\"Ö\": \"\\\\\\\"O \", u\"Ü\": \"\\\\\\\"U \", u\"Ÿ\": \"\\\\\\\"Y \",\n",
    "    u\"ç\": \"\\\\cc \", u\"Ç\": \"\\\\cC \", u\"œ\": \"\\\\oe \", u\"Œ\": \"\\\\OE \", u\"æ\": \"\\\\ae \", u\"Æ\": \"\\\\AE \",\n",
    "    u\"å\": \"\\\\aa \", u\"Å\": \"\\\\AA \", u\"ø\": \"\\\\o \", u\"Ø\": \"\\\\O \", u\"ß\": \"\\\\ss \", u\"¡\": \"!`\", u\"¿\": \"?`\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1fb92c9-57f2-4fe7-9bca-e745bfdb4b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latexAccents2 = {\n",
    "    u\"à\": \"\\\\`a \", # Grave accent\n",
    "    u\"è\": \"\\\\`e \",\n",
    "    u\"ì\": \"\\\\`\\\\i \",\n",
    "    u\"ò\": \"\\\\`o \",\n",
    "    u\"ù\": \"\\\\`u \",\n",
    "    u\"ỳ\": \"\\\\`y \",\n",
    "    u\"À\": \"\\\\`A \",\n",
    "    u\"È\": \"\\\\`E \",\n",
    "    u\"Ì\": \"\\\\`\\\\I \",\n",
    "    u\"Ò\": \"\\\\`O \",\n",
    "    u\"Ù\": \"\\\\`U \",\n",
    "    u\"Ỳ\": \"\\\\`Y \",\n",
    "    u\"á\": \"\\\\'a \", # Acute accent\n",
    "    u\"é\": \"\\\\'e \",\n",
    "    u\"í\": \"\\\\'\\\\i \",\n",
    "    u\"ó\": \"\\\\'o \",\n",
    "    u\"ú\": \"\\\\'u \",\n",
    "    u\"ý\": \"\\\\'y \",\n",
    "    u\"Á\": \"\\\\'A \",\n",
    "    u\"É\": \"\\\\'E \",\n",
    "    u\"Í\": \"\\\\'\\\\I \",\n",
    "    u\"Ó\": \"\\\\'O \",\n",
    "    u\"Ú\": \"\\\\'U \",\n",
    "    u\"Ý\": \"\\\\'Y \",\n",
    "    u\"â\": \"\\\\^a \", # Circumflex\n",
    "    u\"ê\": \"\\\\^e \",\n",
    "    u\"î\": \"\\\\^\\\\i \",\n",
    "    u\"ô\": \"\\\\^o \",\n",
    "    u\"û\": \"\\\\^u \",\n",
    "    u\"ŷ\": \"\\\\^y \",\n",
    "    u\"Â\": \"\\\\^A \",\n",
    "    u\"Ê\": \"\\\\^E \",\n",
    "    u\"Î\": \"\\\\^\\\\I \",\n",
    "    u\"Ô\": \"\\\\^O \",\n",
    "    u\"Û\": \"\\\\^U \",\n",
    "    u\"Ŷ\": \"\\\\^Y \",\n",
    "    u\"ä\": \"\\\\\\\"a \",    # Umlaut or dieresis\n",
    "    u\"ë\": \"\\\\\\\"e \",\n",
    "    u\"ï\": \"\\\\\\\"\\\\i \",\n",
    "    u\"ö\": \"\\\\\\\"o \",\n",
    "    u\"ü\": \"\\\\\\\"u \",\n",
    "    u\"ÿ\": \"\\\\\\\"y \",\n",
    "    u\"Ä\": \"\\\\\\\"A \",\n",
    "    u\"Ë\": \"\\\\\\\"E \",\n",
    "    u\"Ï\": \"\\\\\\\"\\\\I \",\n",
    "    u\"Ö\": \"\\\\\\\"O \",\n",
    "    u\"Ü\": \"\\\\\\\"U \",\n",
    "    u\"Ÿ\": \"\\\\\\\"Y \",\n",
    "    u\"ç\": \"\\\\cc \",   # Cedilla\n",
    "    u\"Ç\": \"\\\\cC \",\n",
    "    u\"œ\": \"\\\\oe \",   # Ligatures\n",
    "    u\"Œ\": \"\\\\OE \",\n",
    "    u\"æ\": \"\\\\ae \",\n",
    "    u\"Æ\": \"\\\\AE \",\n",
    "    u\"å\": \"\\\\aa \",\n",
    "    u\"Å\": \"\\\\AA \",\n",
    "    u\"ø\": \"\\\\o \",    # Misc latin-1 letters\n",
    "    u\"Ø\": \"\\\\O \",\n",
    "    u\"ß\": \"\\\\ss \",\n",
    "    u\"¡\": \"!`\" ,\n",
    "    u\"¿\": \"?`\" ,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f8dd655-efc7-48bb-baa3-39ff7a0a039e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(latexAccents))\n",
    "len(list(latexAccents2))\n",
    "for x in list(latexAccents): assert x in list(latexAccents2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6eda2c-3ac9-4c7c-93d7-1a8e210f3f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans_gpu",
   "language": "python",
   "name": "trans_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
