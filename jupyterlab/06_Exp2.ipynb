{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec30feaa-9aa4-4c09-aee1-59d2a3b2d45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt2-xl', 'gpt2-xl-untrained_1', 'gpt2', 'gpt2-untrained_1', 'gpt2-untrained_2', 'gpt2-untrained_3', 'gpt2-untrained_4', 'gpt2-untrained_5', 'gpt2-untrained_6', 'gpt2-untrained_7', 'gpt2-untrained_8', 'gpt2-untrained_9', 'gpt2-untrained_1_weight_config_all', 'gpt2-untrained_2_weight_config_all', 'gpt2-untrained_3_weight_config_all', 'gpt2-untrained_4_weight_config_all', 'gpt2-untrained_5_weight_config_all', 'gpt2-untrained_6_weight_config_all', 'gpt2-untrained_7_weight_config_all', 'gpt2-untrained_8_weight_config_all', 'gpt2-untrained_9_weight_config_all']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7f60859d574805a8825df947d8cbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading mydatadict:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659911f5821745a8a9aa73783e821a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading models:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./Preamble.ipynb\n",
    "\n",
    "all_models = PM.check_for_existing_patterns(\"activations\")\n",
    "# Uncomment below lines to use specific model sets:\n",
    "# all_models=PM.gpt2xl_models\n",
    "# all_models=PM.gpt2_models\n",
    "# all_models=['gpt2','gpt2-untrained_1','gpt2-untrained_1_weight_config_all']\n",
    "print(all_models)\n",
    "\n",
    "## load preprocessed data\n",
    "ys_6 = PM.load_ys(compress_to_POS_6=True, v=0) # , use_cache=False)\n",
    "Xss_6 = PM.load_Xss(all_models, compress_to_POS_6=True, v=0) # , use_cache=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb579621-d249-4102-bf75-c98d406c4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = \"exp2\" # experiment folder code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c2a7d17-d2f2-41dd-a70c-e2a13a9148c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "def create_splits(n1=10, n2=8, n3=5):\n",
    "    \"\"\"\n",
    "    Create randomized splits for training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        n1 (int): Number of training samples.\n",
    "        n2 (int): Number of validation samples.\n",
    "        n3 (int): Number of test samples.\n",
    "\n",
    "    Returns:\n",
    "        list: Sorted indices for each split.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(1)\n",
    "    n = n1 + n2 + n3\n",
    "    splits = np.split(rng.permutation(n), (n1, n1+n2))\n",
    "    return [np.sort(x) for x in splits]\n",
    "\n",
    "def generate_tvt_inds(cat_inds, tvt_splits):\n",
    "    \"\"\"\n",
    "    Generate indices for training, validation, and test splits based on categories.\n",
    "\n",
    "    Args:\n",
    "        cat_inds (list): List of indices for each category.\n",
    "        tvt_splits (list): List of splits for training, validation, and test.\n",
    "\n",
    "    Returns:\n",
    "        list: Indices for training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    choices_inds = []\n",
    "    for inds in cat_inds:\n",
    "        rng = np.random.default_rng(1)\n",
    "        choice = rng.choice(inds, sum(len(x) for x in tvt_splits), replace=False)\n",
    "        choices_inds.append(choice)\n",
    "    return ([np.sort(x.take(split, axis=0)) for x in choices_inds] for split in tvt_splits)\n",
    "\n",
    "def reduce_Xss_6(Xss_6, X_inds_set):\n",
    "    \"\"\"\n",
    "    Reduce Xss_6 activations to the selected indices for each set.\n",
    "\n",
    "    Args:\n",
    "        Xss_6 (dict): Dictionary of activations.\n",
    "        X_inds_set (list): List of indices to reduce to.\n",
    "\n",
    "    Returns:\n",
    "        dict: Reduced activations dictionary.\n",
    "    \"\"\"\n",
    "    Xss_6_set = {}\n",
    "    for model, Xs_6 in Xss_6.items():\n",
    "        Xss_6_set_i = {}\n",
    "        for layer, X_6 in Xs_6.items():\n",
    "            list_of_values = []\n",
    "            for inds in X_inds_set:\n",
    "                X_6i = X_6.take(inds, axis=0).T\n",
    "                list_of_values.append(X_6i)\n",
    "            Xss_6_set_i[layer] = list_of_values\n",
    "        Xss_6_set[model] = Xss_6_set_i\n",
    "    return Xss_6_set\n",
    "\n",
    "# Helper function to return a default value\n",
    "def def_value():\n",
    "    def _def_value():\n",
    "        return [None] * 6\n",
    "    return defaultdict(_def_value)\n",
    "\n",
    "# Prepare data splits and reduce activations\n",
    "if len(list(Xss_6.keys())):\n",
    "    num_train = 200\n",
    "    num_valid = 100\n",
    "    num_testt = 75\n",
    "    num_all = num_train + num_valid + num_testt\n",
    "    \n",
    "    # Create splits for train, validation, and test sets\n",
    "    tvt_splits = create_splits(num_train, num_valid, num_testt)\n",
    "    \n",
    "    # Categorize indices based on POS tags\n",
    "    cat_inds = [[i for i, v in enumerate(ys_6[\"POS_7_id\"] == x) if v] for x in range(6)]\n",
    "    \n",
    "    # Generate indices for train, validation, and test sets\n",
    "    X_inds_train, X_inds_valid, X_inds_test = generate_tvt_inds(cat_inds, tvt_splits)\n",
    "    \n",
    "    # Combine all indices and reduce activations\n",
    "    all_inds = [np.sort(list(x) + list(y) + list(z)) for x, y, z in zip(X_inds_train, X_inds_valid, X_inds_test)]\n",
    "    Xss_6_all = reduce_Xss_6(Xss_6, all_inds)\n",
    "    \n",
    "    # Clean up original activations dictionary\n",
    "    for k in list(Xss_6.keys()):\n",
    "        del Xss_6[k]\n",
    "\n",
    "\n",
    "# Normalize activations and split them into train, validation, and test sets\n",
    "if len(list(Xss_6_all.keys())):\n",
    "    Xss_6_train = defaultdict(def_value)\n",
    "    Xss_6_valid = defaultdict(def_value)\n",
    "    Xss_6_test = defaultdict(def_value)\n",
    "    \n",
    "    for model, Xs_6 in Xss_6_all.items():\n",
    "        for layer, X_6 in Xs_6.items():\n",
    "            X_6_concat = np.concatenate(X_6, axis=1)\n",
    "            zscored = zscore(X_6_concat, axis=1)\n",
    "            \n",
    "            for i in range(6):\n",
    "                i1, i2 = num_all * i, num_all * (i + 1)\n",
    "                zscored_part = zscored[:, i1:i2]\n",
    "                Xss_6_train[model][layer][i] = zscored_part[:, :num_train]\n",
    "                Xss_6_valid[model][layer][i] = zscored_part[:, num_train:num_train + num_valid]\n",
    "                Xss_6_test[model][layer][i] = zscored_part[:, num_train + num_valid:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400d46e0-a8be-4af4-be7d-4c4f543593cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68e74509eed4ace9526c6af4921be73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8ceaa820434bfaa35ac334095d6762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2e3192f51943c2bfb0a428438cb5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_master_activations_df(split_names=[\"Xss_6_train\", \"Xss_6_valid\", \"Xss_6_test\"], splits=[Xss_6_train, Xss_6_valid, Xss_6_test]):\n",
    "    \"\"\"\n",
    "    Create a master DataFrame for all activations across different splits.\n",
    "\n",
    "    Args:\n",
    "        split_names (list): List of split names.\n",
    "        splits (list): List of activations splits.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Master DataFrame containing all activations.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for split_name, split in zip(split_names, splits):\n",
    "        for model, Xs_6 in tqdm(split.items()):\n",
    "            for layer, X_6_splits in Xs_6.items():\n",
    "                for cat_ind, X_6 in enumerate(X_6_splits):\n",
    "                    for neuron_id, activations in enumerate(X_6):\n",
    "                        data_list.append([split_name, model, layer, cat_ind, neuron_id, activations])\n",
    "\n",
    "    columns = [\"split_name\", \"model\", \"layer\", \"cat_ind\", \"neuron_id\", \"activations\"]\n",
    "    master_activations_df = pd.DataFrame(data_list, columns=columns)\n",
    "    list_of_tuples = [tuple(x) for x in master_activations_df[[\"split_name\", \"model\", \"layer\", \"cat_ind\", \"neuron_id\"]].values]\n",
    "    master_activations_df = master_activations_df.set_index(pd.MultiIndex.from_tuples(list_of_tuples, names=(\"split_name\", \"model\", \"layer\", \"cat_ind\", \"neuron_id\")))\n",
    "    return master_activations_df\n",
    "\n",
    "# Generate master activations DataFrame and save it\n",
    "master_activations_df = get_master_activations_df()\n",
    "master_activations_df.to_pickle(PM.directories[exp] / f\"master_activations_df_quicksave.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0d3eb8-7e3a-4bdd-ac8d-b7a89102da1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac945c3953d44f698abb4690d7593c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import ranksums\n",
    "\n",
    "def calc_all_rank_sums(Xss_6_train, use_cache=True):\n",
    "    \"\"\"\n",
    "    Calculate rank sums for all layers and neurons in the training set.\n",
    "\n",
    "    Args:\n",
    "        Xss_6_train (dict): Activations from the training set.\n",
    "        use_cache (bool, optional): Use cached results if available.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing rank sums for all layers and neurons.\n",
    "    \"\"\"\n",
    "    def calc_layer_rank_sums(model, layer, layer_activations, use_cache=True):\n",
    "        \"\"\"\n",
    "        Calculate rank sums for each layer's activations.\n",
    "        \"\"\"\n",
    "        layer_save_name = f\"ranksum_layers_{model=},{layer=}.csv\"\n",
    "        if use_cache and (layer_df := PM.load_data(exp, layer_save_name)) is not None:\n",
    "            return layer_df\n",
    "        neuron_dfs = []\n",
    "        for neuron_idx in tqdm(range(len(layer_activations[0])), position=2, leave=False, mininterval=0.4):\n",
    "            neuron_activations = [x[neuron_idx] for x in layer_activations]\n",
    "            neuron_df = calc_neuron_rank_sums(model, layer, neuron_idx, neuron_activations, use_cache=use_cache)\n",
    "            neuron_dfs.append(neuron_df)\n",
    "        layer_df = pd.concat(neuron_dfs)\n",
    "        PM.save_data(exp, layer_save_name, layer_df)\n",
    "        return layer_df\n",
    "\n",
    "    def calc_neuron_rank_sums(model, layer, neuron_idx, neuron_activations, use_cache=True):\n",
    "        \"\"\"\n",
    "        Calculate rank sums for a specific neuron's activations.\n",
    "        \"\"\"\n",
    "        neuron_save_name = f\"ranksums_{model=},{layer=},{neuron_idx=}.csv\"\n",
    "        neuron_df = calc_rank_sums_for_neuron(model, layer, neuron_idx, neuron_activations)\n",
    "        return neuron_df\n",
    "\n",
    "    def calc_rank_sums_for_neuron(model, layer, neuron_idx, neuron_activations):\n",
    "        \"\"\"\n",
    "        Perform rank sum tests between categories for a given neuron.\n",
    "        \"\"\"\n",
    "        i_s, j_s, r_s, p_s = [], [], [], []\n",
    "        cached_results = {i: {j: (None, None) for j in range(6) if not i == j} for i in range(6)}\n",
    "        idx_of_max = np.argmax(np.mean(neuron_activations, axis=1))\n",
    "        \n",
    "        for i in range(6):\n",
    "            for j in range(6):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                r, p = cached_results[i][j]\n",
    "                if p is None:\n",
    "                    if idx_of_max == i or idx_of_max == j:\n",
    "                        r, p = tuple(ranksums(neuron_activations[i], neuron_activations[j]))\n",
    "                    else:\n",
    "                        r, p = (0, 1)  # same average, no certainty\n",
    "                    cached_results[i][j] = (r, p)\n",
    "                    cached_results[j][i] = (-r, p)\n",
    "                i_s.append(i)\n",
    "                j_s.append(j)\n",
    "                r_s.append(r)\n",
    "                p_s.append(p)\n",
    "        \n",
    "        res_df = pd.DataFrame.from_dict({\n",
    "            \"model\": [model]*len(i_s),\n",
    "            \"layer\": [layer]*len(i_s),\n",
    "            \"neuron_idx\": [neuron_idx]*len(i_s),\n",
    "            \"cat1\": i_s,\n",
    "            \"cat2\": j_s,\n",
    "            \"r\": r_s,\n",
    "            \"p\": p_s\n",
    "        })\n",
    "        return res_df\n",
    "\n",
    "    # Process rank sums for all models and layers\n",
    "    layer_dfs = []\n",
    "    for model, Xs in tqdm(Xss_6_train.items(), position=0, leave=True):\n",
    "        for layer, neuron_activations in tqdm(Xs.items(), position=1, leave=False):\n",
    "            layer_dfs.append(calc_layer_rank_sums(model, layer, neuron_activations, use_cache=use_cache))\n",
    "    return pd.concat(layer_dfs)\n",
    "\n",
    "# Load master activations DataFrame and calculate rank sums\n",
    "master_activations_df = pd.read_pickle(PM.directories[exp] / f\"master_activations_df_quicksave.pkl\")\n",
    "rank_sums_df = calc_all_rank_sums(Xss_6_train, use_cache=True)\n",
    "PM.save_data(exp, f\"rank_sums_df_quicksave.csv\", rank_sums_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcd46c16-7e5f-4524-82a2-e0eee7a89b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c5dde56fce49879c1e9e06f98d9fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/346496 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calc_feature_selectivity_pmax(rank_sums_df):\n",
    "    \"\"\"\n",
    "    Calculate feature selectivity based on the p-value max from the rank sums.\n",
    "\n",
    "    Args:\n",
    "        rank_sums_df (pd.DataFrame): DataFrame containing rank sums.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing feature selectivity values.\n",
    "    \"\"\"\n",
    "    res_list = []\n",
    "    idx_labels = [\"model\", \"layer\", \"neuron_idx\"]\n",
    "    \n",
    "    for idx, df in tqdm(rank_sums_df.groupby(idx_labels)):\n",
    "        columns = idx_labels.copy()\n",
    "        model, layer, neuron_idx = idx\n",
    "        res_i = [*idx]\n",
    "        \n",
    "        for cat1, df2 in df.groupby(\"cat1\"):\n",
    "            res_i.append(df2[\"p\"].max())\n",
    "            columns.append(f\"maxp_{cat1}\")\n",
    "            res_i.append(df2[\"r\"].min())\n",
    "            columns.append(f\"minr_{cat1}\")\n",
    "        res_list.append(res_i)\n",
    "    \n",
    "    res = pd.DataFrame(res_list, columns=columns)\n",
    "    return res[[\"model\", \"layer\", \"neuron_idx\", \"maxp_0\", \"maxp_1\", \"maxp_2\", \"maxp_3\", \"maxp_4\", \"maxp_5\", \"minr_0\", \"minr_1\", \"minr_2\", \"minr_3\", \"minr_4\", \"minr_5\"]]\n",
    "\n",
    "# Load rank sums and calculate feature selectivity\n",
    "rank_sums_df = PM.load_data(exp, f\"rank_sums_df_quicksave.csv\")\n",
    "FS_pmax_df = calc_feature_selectivity_pmax(rank_sums_df)\n",
    "PM.save_data(exp, f\"FS_pmax_df_quicksave.csv\", FS_pmax_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cef6f73-1193-49bc-a8f5-2f425284570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_feature_selectivity_df(FS_pmax_df, thresh=10**-3, only_largest=True):\n",
    "    \"\"\"\n",
    "    Calculate feature selectivity and filter neurons based on a threshold.\n",
    "\n",
    "    Args:\n",
    "        FS_pmax_df (pd.DataFrame): DataFrame containing p-max values for feature selectivity.\n",
    "        thresh (float, optional): P-value threshold for selectivity (default is 10^-3).\n",
    "        only_largest (bool, optional): If True, only consider largest min_r values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with feature selectivity.\n",
    "    \"\"\"\n",
    "    res_df = FS_pmax_df.copy()\n",
    "    res_df['neuron_idx'] = res_df['neuron_idx'].astype(str)\n",
    "    \n",
    "    for i in range(6):\n",
    "        if only_largest:\n",
    "            res_df[f\"sel_{i}\"] = ((res_df[f\"maxp_{i}\"] < thresh) & (res_df[f\"minr_{i}\"] > 0)).replace({True: 1, False: 0})\n",
    "        else:\n",
    "            res_df[f\"sel_{i}\"] = (res_df[f\"maxp_{i}\"] < thresh).replace({True: 1, False: 0})\n",
    "        res_df = res_df.drop(columns=[f\"maxp_{i}\", f\"minr_{i}\"])\n",
    "    \n",
    "    res_df['neuron_idx'] = res_df['neuron_idx'].astype(int)\n",
    "    res_df[\"total_sels\"] = res_df[\"sel_0\"] + res_df[\"sel_1\"] + res_df[\"sel_2\"] + res_df[\"sel_3\"] + res_df[\"sel_4\"] + res_df[\"sel_5\"]\n",
    "    return res_df\n",
    "\n",
    "# Load feature selectivity data and calculate selectivity\n",
    "FS_pmax_df = PM.load_data(exp, f\"FS_pmax_df_quicksave.csv\")\n",
    "FS_df = calc_feature_selectivity_df(FS_pmax_df, thresh=10**-3)\n",
    "PM.save_data(exp, f\"FS_df_quicksave.csv\", FS_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d9231d6-eb0b-49e9-a6d3-27f980b4745e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4845bbd1d07a4dd5a80f80a1a506bbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc109caced440b480ee01b8665e1c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calc_selective_neurons_df(FS_df):\n",
    "    \"\"\"\n",
    "    Calculate selectivity indices (FSI) for neurons based on activations.\n",
    "\n",
    "    Args:\n",
    "        FS_df (pd.DataFrame): DataFrame containing feature selectivity data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: DataFrame of unique selective neurons and multi-selective neurons.\n",
    "    \"\"\"\n",
    "    def calc_FSI(x, y):\n",
    "        \"\"\"\n",
    "        Calculate the feature selectivity index (FSI) between two categories.\n",
    "        \"\"\"\n",
    "        meanx = x.mean()\n",
    "        meany = y.mean()\n",
    "        stdx = x.std()\n",
    "        stdy = y.std()\n",
    "        stdave = (stdx**2 + stdy**2) / 2\n",
    "        return (meanx - meany) / (stdave**0.5)\n",
    "    \n",
    "    def add_FSI_to_selective_neurons(unique_selective_neurons, split_name=\"Xss_6_train\"):\n",
    "        \"\"\"\n",
    "        Add FSI values to the selective neurons DataFrame.\n",
    "        \"\"\"\n",
    "        FSIs = []\n",
    "        for (model, layer, neuron_idx, cat_ind), df in tqdm(unique_selective_neurons.groupby([\"model\", \"layer\", \"neuron_idx\", \"cat_ind\"])):\n",
    "            assert len(df) == 1, \"should only have 1 matching value\"\n",
    "            selective_category = master_activations_df.loc[split_name, model, layer, cat_ind, neuron_idx][\"activations\"]\n",
    "            other_categories = np.array([master_activations_df.loc[split_name, model, layer, i, neuron_idx][\"activations\"] for i in range(6) if i != cat_ind])\n",
    "            FSIs.append(calc_FSI(selective_category, other_categories))\n",
    "        return FSIs\n",
    "    \n",
    "    # Process feature selectivity for neurons\n",
    "    selective_neurons = []\n",
    "    for i, sel in enumerate(sels):\n",
    "        sel_inds = FS_df[FS_df[sel] == 1][[\"model\", \"layer\", \"neuron_idx\", \"total_sels\"]]\n",
    "        sel_inds[\"cat_ind\"] = i\n",
    "        sel_inds = sel_inds[[\"model\", \"layer\", \"neuron_idx\", \"cat_ind\", \"total_sels\"]]\n",
    "        selective_neurons.append(sel_inds)\n",
    "    \n",
    "    selective_neurons = pd.concat(selective_neurons, ignore_index=True)\n",
    "    multi_selective_neurons = selective_neurons[selective_neurons[\"total_sels\"] != 1]\n",
    "    unique_selective_neurons = selective_neurons[selective_neurons[\"total_sels\"] == 1].drop(columns=[\"total_sels\"])\n",
    "    unique_selective_neurons[\"FSI\"] = add_FSI_to_selective_neurons(unique_selective_neurons, split_name=\"Xss_6_train\")\n",
    "    unique_selective_neurons[\"FSI_valid\"] = add_FSI_to_selective_neurons(unique_selective_neurons, split_name=\"Xss_6_valid\")\n",
    "    \n",
    "    return unique_selective_neurons, multi_selective_neurons\n",
    "\n",
    "# Load feature selectivity data and calculate selective neurons\n",
    "FS_df = PM.load_data(exp, f\"FS_df_quicksave.csv\")\n",
    "unique_selective_neurons, _ = calc_selective_neurons_df(FS_df)\n",
    "PM.save_data(exp, f\"unique_selective_neurons_quicksave.csv\", unique_selective_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d385fe-6f55-4080-8a89-a7c18c0b493f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans_gpu",
   "language": "python",
   "name": "trans_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
